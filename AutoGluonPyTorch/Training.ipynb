{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62d83f53-4aa5-49d4-b7b3-6bccd7831b2b",
   "metadata": {},
   "source": [
    "### Use the AG example in the docs (Forecasting with Chronos) and then convert to training container and inf container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "69a575bc-12e6-477d-8f8b-d17901927ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n",
    "import numpy as np\n",
    "import boto3\n",
    "import uuid\n",
    "from io import BytesIO\n",
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.session import TrainingInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d420423b-53bc-424e-aa29-04bb54267277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">T000000</th>\n",
       "      <th>2013-03-10 00:00:00</th>\n",
       "      <td>5207.959961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-10 00:30:00</th>\n",
       "      <td>5002.275879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-10 01:00:00</th>\n",
       "      <td>4747.569824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-10 01:30:00</th>\n",
       "      <td>4544.880859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-10 02:00:00</th>\n",
       "      <td>4425.952148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  target\n",
       "item_id timestamp                       \n",
       "T000000 2013-03-10 00:00:00  5207.959961\n",
       "        2013-03-10 00:30:00  5002.275879\n",
       "        2013-03-10 01:00:00  4747.569824\n",
       "        2013-03-10 01:30:00  4544.880859\n",
       "        2013-03-10 02:00:00  4425.952148"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = TimeSeriesDataFrame.from_path(\n",
    "    \"https://autogluon.s3.amazonaws.com/datasets/timeseries/australian_electricity_subset/test.csv\"\n",
    ")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cab1d3-92ca-47e3-884c-35637afa4531",
   "metadata": {},
   "source": [
    "### Add features (not in ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d64d03e6-c91b-4c31-9cda-275cfd11abe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>random_feature</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">T000000</th>\n",
       "      <th>2013-03-10 00:00:00</th>\n",
       "      <td>5207.959961</td>\n",
       "      <td>5558.006640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-10 00:30:00</th>\n",
       "      <td>5002.275879</td>\n",
       "      <td>5390.543877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-10 01:00:00</th>\n",
       "      <td>4747.569824</td>\n",
       "      <td>4798.287882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-10 01:30:00</th>\n",
       "      <td>4544.880859</td>\n",
       "      <td>4545.228370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-10 02:00:00</th>\n",
       "      <td>4425.952148</td>\n",
       "      <td>4706.047467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  target  random_feature\n",
       "item_id timestamp                                       \n",
       "T000000 2013-03-10 00:00:00  5207.959961     5558.006640\n",
       "        2013-03-10 00:30:00  5002.275879     5390.543877\n",
       "        2013-03-10 01:00:00  4747.569824     4798.287882\n",
       "        2013-03-10 01:30:00  4544.880859     4545.228370\n",
       "        2013-03-10 02:00:00  4425.952148     4706.047467"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_values = np.random.uniform(0, 500, size=len(data['target']))\n",
    "data['random_feature'] = data['target'].values + random_values\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8349eed-dc1f-4820-8017-28f37030bc46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prediction_length = 48\n",
    "# train_data, test_data = data.train_test_split(prediction_length)\n",
    "# train_data = train_data.reset_index()\n",
    "# test_data = test_data.reset_index()\n",
    "\n",
    "# predictor = TimeSeriesPredictor(prediction_length=prediction_length\n",
    "#                                ).fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a930493-6870-422b-bda7-51ae9e96e79a",
   "metadata": {},
   "source": [
    "### Write files to parquet (not in ex.) to test train and inf job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "183acd3d-ef66-4f3a-ac08-87921ac03ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'ag-example-timeseries'\n",
    "\n",
    "# Create S3 client\n",
    "s3 = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "94bc8505-31f0-4d9c-aa9e-560239998d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>target</th>\n",
       "      <th>random_feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T000000</td>\n",
       "      <td>2013-03-10 00:00:00</td>\n",
       "      <td>5207.959961</td>\n",
       "      <td>5558.006640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T000000</td>\n",
       "      <td>2013-03-10 00:30:00</td>\n",
       "      <td>5002.275879</td>\n",
       "      <td>5390.543877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T000000</td>\n",
       "      <td>2013-03-10 01:00:00</td>\n",
       "      <td>4747.569824</td>\n",
       "      <td>4798.287882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T000000</td>\n",
       "      <td>2013-03-10 01:30:00</td>\n",
       "      <td>4544.880859</td>\n",
       "      <td>4545.228370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T000000</td>\n",
       "      <td>2013-03-10 02:00:00</td>\n",
       "      <td>4425.952148</td>\n",
       "      <td>4706.047467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172795</th>\n",
       "      <td>T000004</td>\n",
       "      <td>2015-02-27 21:30:00</td>\n",
       "      <td>368.948792</td>\n",
       "      <td>452.005475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172796</th>\n",
       "      <td>T000004</td>\n",
       "      <td>2015-02-27 22:00:00</td>\n",
       "      <td>346.332764</td>\n",
       "      <td>598.047604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172797</th>\n",
       "      <td>T000004</td>\n",
       "      <td>2015-02-27 22:30:00</td>\n",
       "      <td>327.962677</td>\n",
       "      <td>499.156166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172798</th>\n",
       "      <td>T000004</td>\n",
       "      <td>2015-02-27 23:00:00</td>\n",
       "      <td>307.481934</td>\n",
       "      <td>498.390110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172799</th>\n",
       "      <td>T000004</td>\n",
       "      <td>2015-02-27 23:30:00</td>\n",
       "      <td>291.532776</td>\n",
       "      <td>588.931326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>172800 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        item_id           timestamp       target  random_feature\n",
       "0       T000000 2013-03-10 00:00:00  5207.959961     5558.006640\n",
       "1       T000000 2013-03-10 00:30:00  5002.275879     5390.543877\n",
       "2       T000000 2013-03-10 01:00:00  4747.569824     4798.287882\n",
       "3       T000000 2013-03-10 01:30:00  4544.880859     4545.228370\n",
       "4       T000000 2013-03-10 02:00:00  4425.952148     4706.047467\n",
       "...         ...                 ...          ...             ...\n",
       "172795  T000004 2015-02-27 21:30:00   368.948792      452.005475\n",
       "172796  T000004 2015-02-27 22:00:00   346.332764      598.047604\n",
       "172797  T000004 2015-02-27 22:30:00   327.962677      499.156166\n",
       "172798  T000004 2015-02-27 23:00:00   307.481934      498.390110\n",
       "172799  T000004 2015-02-27 23:30:00   291.532776      588.931326\n",
       "\n",
       "[172800 rows x 4 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "55e42fb1-e4f1-428b-b3f1-bd2458d77bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "# Number of parquet files to create\n",
    "num_files = 100\n",
    "\n",
    "dfs_to_write = {'train': train_data, 'test': test_data}\n",
    "\n",
    "for key in dfs_to_write.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "9316b046-ef8c-417c-a19a-5903a440809b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://ag-example-timeseries/train/dummy_051a6370.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_074ddab6.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_05246c34.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_063c3e10.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_09ea8721.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_11711ae8.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_08b1dc91.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_0941929d.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_11a2f2fd.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_1228d43d.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_11fd6d93.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_199f7825.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_16b90365.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_150b9b3e.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_19db2211.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_10471392.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_0fb63b0a.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_1a95312d.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_0ffe1391.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_1a75da24.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_1c1fc420.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_1ac993dd.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_1bbdcb62.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_1d593185.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_1ddcabe9.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_1dfadc41.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_21d8cd9c.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_24e7b9ab.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_0fcb28fe.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_1f0296a8.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_1fd9d98d.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_27dbcb3f.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_28c0ef4e.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_287cde9f.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_2211bd71.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_29acd190.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_29ebfcf1.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_2c1e17bd.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_2816f5a0.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_2e7cdc6f.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_2cb0fe02.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_2f2ffc12.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_2d811563.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_2e8d7323.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_2f4b5188.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_333bfa3e.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_33a1f0bb.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_37a7eeb3.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_34f3218d.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_3ad15522.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_39371b46.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_3c44e192.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_3f6b206f.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_3f9ff9b7.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_42a087be.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_42d11a4a.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_43949a5b.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_43c43d6b.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_48cb57e2.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_4ac561a8.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_4b895e23.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_4c10bbdf.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_51831475.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_4f32bbf0.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_5083602a.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_4a6d449c.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_5368598a.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_52f216eb.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_542d88d4.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_53a4be1d.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_55088c3b.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_5804aff3.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_5bbb3399.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_5ddcd6f5.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_5ea14ada.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_18b66dd9.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_5ef7709c.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_603eeb25.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_63779f74.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_62b1f11a.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_64764251.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_6567c5ab.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_66f72cc0.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_66fc44a8.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_67347828.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_67ac3736.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_5c260136.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_69d9553d.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_6b9d4050.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_6435c069.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_6c3d5716.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_6cb82bdf.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_70203ca7.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_70503b17.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_70a0f31e.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_762a5997.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_72ce9187.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_7c6616b6.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_808f209e.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_7118e3d0.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_7d529b48.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_7e910ad6.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_81b070c6.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_81cec5b5.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_7cd709c6.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_885d89f6.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_8a937486.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_88fc16de.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_8f49de82.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_841f2bc0.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_8fa05b01.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_93278547.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_92d8bf19.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_936011f0.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_93b1d0ac.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_90e8bd36.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_8bf2ceea.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_9c9f21c0.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_989abacf.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_9912f37a.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_9a764ea8.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_9d296cac.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_9e6d3f7f.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_9ddef1b9.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_9db06450.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_9ebd869a.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_a3bad49f.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_a10afb93.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_17ef5cc9.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_ac7f0eb2.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_acb07ec2.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_aa287af4.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_ad341363.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_ae00e515.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_af040c68.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_afd96b94.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_b314c8f8.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_b28cdc35.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_b524a010.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_b358e145.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_b5ec04b8.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_b3a07756.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_b465f1ad.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_b7d0ead8.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_b8fdfe6c.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_b91a0f34.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_baea8b9c.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_bd6a3bf5.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_bd90d875.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_ba6c093c.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_be332ffa.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_bd91c9fb.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_c0ae05e8.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_beec99bc.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_bf0b0f1d.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_c212470a.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_c2b69537.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_c3f603b0.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_c3eb8da8.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_c4de3898.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_c6dd1fd6.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_c7465267.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_c77e0007.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_cc021813.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_cf9b4b35.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_d0c07150.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_d1a91ec5.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_d35c155a.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_d3fa8693.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_d57fd738.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_d5e7fabe.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_d4478378.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_d70b19b9.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_dbd33219.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_e11ae8db.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_db31d0de.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_e25ba004.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_e42bdf0c.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_e29e9280.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_e49ea580.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_e9a3d22b.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_e97c0b04.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_ecfdf123.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_ea12f48b.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_ef98f533.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_ef17cb87.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_f482c17b.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_5da9f52f.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_17bc78fc.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_f7d29dcc.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_a41d8d3a.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_1788eae3.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_f832c1c0.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_f852b993.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_fea4c1e1.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_ff391d2b.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_fe62e31a.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_fd801251.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_ff3a32b2.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_fc02f0c4.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_000940ff.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_010a9858.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_014a43a4.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_01f14faf.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_063af385.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_023e9270.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_07a3bc28.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_09ea5b1b.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_0b6d66af.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_048d6dd1.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_0303f1d7.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_11305dcb.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_1270268c.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_05d180ea.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_12ba3bbc.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_1345a3fa.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_06a406fe.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_077af902.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_13adb48d.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_162d992d.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_19f8a551.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_1bf58705.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_1d731a29.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_1e365f38.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_1e2adbac.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_1ead1f21.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_2027fcbb.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_1ee0db47.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_21ff835a.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_233db148.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_2427db3f.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_24ffbce7.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_253c1da2.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_268c69d7.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_28a7c1b7.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_2ad83190.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_28c72427.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_2b832058.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_2c5df371.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_2ee9406a.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_2c6bbae5.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_30f38160.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_3111b5fb.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_33fec6bf.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_346202a3.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_37a17fec.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_35c03ea0.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_37bfb9dc.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_37c98846.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_38df46be.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_391f7f6a.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_3850981d.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_31c3146a.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_3966c28a.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_3c66c03a.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_3cfb8fba.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_3d72786c.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_3ef20987.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_3fee32a9.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_40624f9f.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_3f7b5b43.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_419aa6e9.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_4209d28c.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_445239a1.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_42d44873.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_48311e38.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_492c1595.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_4cd44d04.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_4a94b012.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_4d6cc5e9.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_4f7becf6.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_4d25d1f1.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_4fab9d0e.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_501b567e.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_5167770c.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_551cb2b8.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_54f2e07a.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_55e52e11.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_5597ce7f.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_1669232e.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_18153726.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_5abba4fb.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_5ac47ad6.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_17286244.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_16a66882.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_5b9c7f17.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_5d44dec0.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_5e09a281.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_5bd7f9cc.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_584397a1.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_605d34d7.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_6da8dc30.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_6b0a7088.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_6f3f01ce.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_6f4be693.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_7454565f.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_76e78ba3.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_7517972a.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_78ed0bd9.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_753acfd8.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_77ae776a.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_7900d465.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_7b008640.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_7d5a691e.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_7f6ae6de.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_7fda13fd.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_80ff278f.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_847de136.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_8174af23.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_882094fa.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_850cae52.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_895afb2f.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_82d8979c.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_84c51e20.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_867cf711.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_8f83ec0b.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_93550ee3.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_9547c90c.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_99c3959b.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_9aefa6da.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_9b62a940.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_a47748bb.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_a3537447.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_9ee3462a.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_95b1e5dd.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_a3e1cef3.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_9f6d375b.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_99a0b348.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_a8ee0419.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_a96708f4.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_ac944ca5.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_ab73542b.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_acc947ba.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_aea8f115.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_afcb9e01.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_a7b618fe.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_b16b7980.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_ae40badc.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_b42ad01a.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_b37ab91a.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_b4d83f08.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_b555a3b5.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_63320093.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_b816400d.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_b95cac0d.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_bcadc09c.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_be551ed3.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_bb32303b.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_c04ce3ad.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_c090562a.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_bd7d9c5a.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_c137d3bc.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_c4db6c30.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_c6c0c889.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_c6c4df48.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_c8535c7c.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_cbf7a4cf.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_ce14490d.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_d11fe124.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_ce5122bb.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_d29b0678.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_d42becbc.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_cef36e1d.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_d586dc7e.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_d8cdb72b.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_dcd73c09.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_dd19f9bd.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_de7c1473.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_dd312388.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_dd43023a.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_e05dae56.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_e2d3bbc5.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_e79342fa.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_e747eb7e.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_e08a5537.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_e5ec1f0d.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_e965c504.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_e92f8b3e.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_e9f4c6b5.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_eba9de3d.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_eaa3d249.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_ef248938.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_eed4ef4e.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_f2150db6.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_eb40365c.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_f0b371c5.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_efe337f8.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_f38023ec.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_f3d25011.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_f2a8e4b0.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_f8239f20.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_f400fcc2.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_f933975c.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_fb2bd8c8.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_6968399c.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_fc690225.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_b75bed34.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_fe7095ee.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_63fef998.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_6542a07a.parquet\n"
     ]
    }
   ],
   "source": [
    "# Clear train & test buckets [can change this]\n",
    "! aws s3 rm s3://{bucket}/train/ --recursive \n",
    "! aws s3 rm s3://{bucket}/test/ --recursive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "04459787-adbc-4dfd-b4d7-cded0eea525e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>target</th>\n",
       "      <th>random_feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>171310</th>\n",
       "      <td>T000003</td>\n",
       "      <td>2014-09-15 19:00:00</td>\n",
       "      <td>1549.564575</td>\n",
       "      <td>1889.001289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171311</th>\n",
       "      <td>T000003</td>\n",
       "      <td>2013-12-27 20:00:00</td>\n",
       "      <td>1408.881348</td>\n",
       "      <td>1707.482654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171312</th>\n",
       "      <td>T000003</td>\n",
       "      <td>2014-11-08 18:30:00</td>\n",
       "      <td>1049.678345</td>\n",
       "      <td>1318.529002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171313</th>\n",
       "      <td>T000001</td>\n",
       "      <td>2014-07-03 06:00:00</td>\n",
       "      <td>4561.230957</td>\n",
       "      <td>4620.362423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171314</th>\n",
       "      <td>T000002</td>\n",
       "      <td>2013-05-31 05:00:00</td>\n",
       "      <td>3342.124268</td>\n",
       "      <td>3776.884811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173035</th>\n",
       "      <td>T000003</td>\n",
       "      <td>2014-02-08 11:30:00</td>\n",
       "      <td>2116.354248</td>\n",
       "      <td>2325.709252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173036</th>\n",
       "      <td>T000002</td>\n",
       "      <td>2015-03-30 07:00:00</td>\n",
       "      <td>4776.804199</td>\n",
       "      <td>5155.161506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173037</th>\n",
       "      <td>T000003</td>\n",
       "      <td>2014-10-17 14:00:00</td>\n",
       "      <td>978.261658</td>\n",
       "      <td>1009.637501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173038</th>\n",
       "      <td>T000004</td>\n",
       "      <td>2013-09-01 17:30:00</td>\n",
       "      <td>500.091858</td>\n",
       "      <td>858.222577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173039</th>\n",
       "      <td>T000003</td>\n",
       "      <td>2014-03-23 19:00:00</td>\n",
       "      <td>1141.209473</td>\n",
       "      <td>1229.102556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1730 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        item_id           timestamp       target  random_feature\n",
       "171310  T000003 2014-09-15 19:00:00  1549.564575     1889.001289\n",
       "171311  T000003 2013-12-27 20:00:00  1408.881348     1707.482654\n",
       "171312  T000003 2014-11-08 18:30:00  1049.678345     1318.529002\n",
       "171313  T000001 2014-07-03 06:00:00  4561.230957     4620.362423\n",
       "171314  T000002 2013-05-31 05:00:00  3342.124268     3776.884811\n",
       "...         ...                 ...          ...             ...\n",
       "173035  T000003 2014-02-08 11:30:00  2116.354248     2325.709252\n",
       "173036  T000002 2015-03-30 07:00:00  4776.804199     5155.161506\n",
       "173037  T000003 2014-10-17 14:00:00   978.261658     1009.637501\n",
       "173038  T000004 2013-09-01 17:30:00   500.091858      858.222577\n",
       "173039  T000003 2014-03-23 19:00:00  1141.209473     1229.102556\n",
       "\n",
       "[1730 rows x 4 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "ae127969-e2c7-46e9-a5d3-02474206e21f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'TimeSeriesDataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'TimeSeriesDataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_ed5a6ade.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_12d36c43.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_45a88326.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_30bd8e8a.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_be566b9b.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_9f66ec89.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_e2423f6f.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_84126629.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_2775cb49.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_b528749f.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_b3b95a92.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_a7c3250e.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_b86b53c8.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_ba9dc133.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_4623a0c6.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_65dbb856.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_ecc12c90.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_3b86768a.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_5060a455.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_53dd828b.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_413f169f.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_784f2b6f.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_85140ff6.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_155dc49c.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_393e6ff2.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_d43d28ac.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_40a9a813.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_f9c32045.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_e4412086.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_2b51bd25.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_001ceaa1.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_382d6df7.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_950207d5.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_4a29f11c.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_75ace04f.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_28764a7c.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_70d2ec8b.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_5cdda393.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_b87f2d1f.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_df7ca611.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_a70ccc6c.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_9d2ff974.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_5547250f.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_75c8fae8.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_a42fac17.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_6aad4ff6.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_239d9a2c.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_4c264491.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_d8c54683.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_8161aef6.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_ebbfa2a8.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_cd669121.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_2f8f196c.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_3c2930dd.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_b70f648f.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_ec460023.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_791b6a74.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_e36b4707.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_0137fab4.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_54905031.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_2228c853.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_0a66dbcf.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_3d412d11.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_e6266ac5.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_29b30b1c.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_68fbab14.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_b9594dea.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_1a499879.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_0cd4de9c.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_60f1ee0a.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_8833dd3c.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_2d818475.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_a2af6118.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_423aeb94.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_edc6a0f9.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_024b7783.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_383df2c9.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_b69bb2c4.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_b2d3b297.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_dfc8a691.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_16c6cf8f.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_42f7c3b5.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_37c9ac80.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_ca08dcff.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_f8989380.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_e2ee0d38.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_f072ba15.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_79b263a3.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_445b1cb1.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_a469b567.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_b2a3add2.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_1b7342c1.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_a8963774.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_d843f78d.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_439042e2.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_fce9924e.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_bf88f94a.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_095f27c6.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_d11c8fdb.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_e1e8e502.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_7dfc4754.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'TimeSeriesDataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'TimeSeriesDataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_8724b0d9.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_7fe16a93.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_66ffae35.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_591feade.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_7a4a578a.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_82f35802.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_1c9f47bb.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_99322d03.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_9cf88685.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_0d2bd7a8.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_1ba8fad4.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_95ea3bb9.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_45210d0f.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_c77b5455.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_46cb590d.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_850a2905.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_a8ff1f8c.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_14da8287.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_bd7ed48b.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_ea1c24b6.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_116ebd18.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_671a160c.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_eed2c6d0.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_d15991dc.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_13f3f935.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_f706aa2b.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_a07acf7a.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_89e19999.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_c76bae3e.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_cd26dc32.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_ef670f91.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_e79d5915.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_29203109.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_ffffd26f.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_9a0c304a.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_f186a75a.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_c2190e33.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_3525c0ea.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_39bd1508.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_e9c2d951.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_813d3c07.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_4520217b.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_b09ce781.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_46501a0b.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_cf986fe1.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_43650ce7.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_0aecef1d.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_502f225f.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_207e143c.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_306dbf32.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_7efde3f8.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_1567dad4.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_af6b25cd.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_8dd1af2d.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_60276d81.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_80f6c6d3.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_bec72384.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_9e4a45f3.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_8b45e74d.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_3de898f7.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_2bf617d0.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_6a62a0a6.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_df8511fd.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_c84c9dba.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_421154f0.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_e3591643.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_7d9cf6dd.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_63c36b7a.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_d830b8c1.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_da71be00.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_8a7f1765.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_cd0f78f7.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_e71d200c.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_293ee6d7.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_15acd154.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_aea1bd7e.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_4aefa8ff.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_0d314a3a.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_9f561aea.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_f1e59210.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_8e2df1c3.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_f76ec868.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_22790e1c.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_9cb14d4a.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_dee6f1b2.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_a59d1dd3.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_371e1f73.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_4b25f8bb.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_c71b4789.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_c647f862.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_481a39bc.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_2bd5ea5d.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_9f4320f3.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_c0ed7861.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_efe96f9a.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_1408f6b9.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_2236ae85.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_511864c7.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_31139dd0.parquet\n"
     ]
    }
   ],
   "source": [
    "for split_name, df in dfs_to_write.items():\n",
    "    # Shuffle and split into chunks\n",
    "    shuffled = df.sample(frac=1, random_state=42).reset_index()\n",
    "    chunks = np.array_split(shuffled, num_files)\n",
    "\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        filename = f\"dummy_{uuid.uuid4().hex[:8]}.parquet\"\n",
    "        s3_key = f\"{split_name}/{filename}\"   # <-- include folder + filename\n",
    "\n",
    "        buffer = BytesIO()\n",
    "        chunk_reset = chunk.reset_index()\n",
    "        chunk_reset.to_parquet(buffer, index=False)\n",
    "        buffer.seek(0)\n",
    "\n",
    "        s3.upload_fileobj(buffer, bucket, s3_key)\n",
    "        print(f\"Uploaded {len(chunk)} rows to s3://{bucket}/{s3_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b1059ada-8ef7-486c-bbc5-fa8a75f8978e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['item_id', 'timestamp', 'target', 'random_feature'], dtype='object')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "925a3230-a6e7-49de-b6e2-edd0f706e628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble\n"
     ]
    }
   ],
   "source": [
    "train_data\n",
    "predictions = predictor.predict(train_data)\n",
    "# predictor.plot(\n",
    "#     data=data,\n",
    "#     predictions=predictions,\n",
    "#     item_ids=data.item_ids[:2],\n",
    "#     max_history_length=200,\n",
    "# );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d7e7bd-6126-474e-bf6f-c7be6c4ea192",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble\n"
     ]
    }
   ],
   "source": [
    "# predictions = predictor.predict(train_data)\n",
    "# predictor.plot(\n",
    "#     data=data,\n",
    "#     predictions=predictions,\n",
    "#     item_ids=data.item_ids[:2],\n",
    "#     max_history_length=200,\n",
    "# );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "f879ed39-9b62-44a0-872d-27718aa7971a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://ag-example-timeseries/train/dummy_0137fab4.parquet to train/dummy_0137fab4.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_0cd4de9c.parquet to train/dummy_0cd4de9c.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_0a66dbcf.parquet to train/dummy_0a66dbcf.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_12d36c43.parquet to train/dummy_12d36c43.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_155dc49c.parquet to train/dummy_155dc49c.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_095f27c6.parquet to train/dummy_095f27c6.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_001ceaa1.parquet to train/dummy_001ceaa1.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_024b7783.parquet to train/dummy_024b7783.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_16c6cf8f.parquet to train/dummy_16c6cf8f.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_1a499879.parquet to train/dummy_1a499879.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_1b7342c1.parquet to train/dummy_1b7342c1.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_2228c853.parquet to train/dummy_2228c853.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_2775cb49.parquet to train/dummy_2775cb49.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_239d9a2c.parquet to train/dummy_239d9a2c.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_28764a7c.parquet to train/dummy_28764a7c.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_2f8f196c.parquet to train/dummy_2f8f196c.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_2b51bd25.parquet to train/dummy_2b51bd25.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_37c9ac80.parquet to train/dummy_37c9ac80.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_3b86768a.parquet to train/dummy_3b86768a.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_382d6df7.parquet to train/dummy_382d6df7.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_30bd8e8a.parquet to train/dummy_30bd8e8a.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_3d412d11.parquet to train/dummy_3d412d11.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_3c2930dd.parquet to train/dummy_3c2930dd.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_29b30b1c.parquet to train/dummy_29b30b1c.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_413f169f.parquet to train/dummy_413f169f.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_2d818475.parquet to train/dummy_2d818475.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_40a9a813.parquet to train/dummy_40a9a813.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_42f7c3b5.parquet to train/dummy_42f7c3b5.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_45a88326.parquet to train/dummy_45a88326.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_423aeb94.parquet to train/dummy_423aeb94.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_4623a0c6.parquet to train/dummy_4623a0c6.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_383df2c9.parquet to train/dummy_383df2c9.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_53dd828b.parquet to train/dummy_53dd828b.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_439042e2.parquet to train/dummy_439042e2.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_54905031.parquet to train/dummy_54905031.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_5547250f.parquet to train/dummy_5547250f.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_5cdda393.parquet to train/dummy_5cdda393.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_60f1ee0a.parquet to train/dummy_60f1ee0a.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_393e6ff2.parquet to train/dummy_393e6ff2.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_4a29f11c.parquet to train/dummy_4a29f11c.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_445b1cb1.parquet to train/dummy_445b1cb1.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_68fbab14.parquet to train/dummy_68fbab14.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_65dbb856.parquet to train/dummy_65dbb856.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_5060a455.parquet to train/dummy_5060a455.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_75c8fae8.parquet to train/dummy_75c8fae8.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_784f2b6f.parquet to train/dummy_784f2b6f.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_8161aef6.parquet to train/dummy_8161aef6.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_79b263a3.parquet to train/dummy_79b263a3.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_85140ff6.parquet to train/dummy_85140ff6.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_84126629.parquet to train/dummy_84126629.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_950207d5.parquet to train/dummy_950207d5.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_9d2ff974.parquet to train/dummy_9d2ff974.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_70d2ec8b.parquet to train/dummy_70d2ec8b.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_4c264491.parquet to train/dummy_4c264491.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_8833dd3c.parquet to train/dummy_8833dd3c.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_791b6a74.parquet to train/dummy_791b6a74.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_9f66ec89.parquet to train/dummy_9f66ec89.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_a7c3250e.parquet to train/dummy_a7c3250e.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_75ace04f.parquet to train/dummy_75ace04f.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_a8963774.parquet to train/dummy_a8963774.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_b2a3add2.parquet to train/dummy_b2a3add2.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_a2af6118.parquet to train/dummy_a2af6118.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_a42fac17.parquet to train/dummy_a42fac17.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_a70ccc6c.parquet to train/dummy_a70ccc6c.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_a469b567.parquet to train/dummy_a469b567.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_b2d3b297.parquet to train/dummy_b2d3b297.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_b3b95a92.parquet to train/dummy_b3b95a92.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_b70f648f.parquet to train/dummy_b70f648f.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_b69bb2c4.parquet to train/dummy_b69bb2c4.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_b86b53c8.parquet to train/dummy_b86b53c8.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_b87f2d1f.parquet to train/dummy_b87f2d1f.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_b9594dea.parquet to train/dummy_b9594dea.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_be566b9b.parquet to train/dummy_be566b9b.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_ba9dc133.parquet to train/dummy_ba9dc133.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_b528749f.parquet to train/dummy_b528749f.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_6aad4ff6.parquet to train/dummy_6aad4ff6.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_ca08dcff.parquet to train/dummy_ca08dcff.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_cd669121.parquet to train/dummy_cd669121.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_d11c8fdb.parquet to train/dummy_d11c8fdb.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_d43d28ac.parquet to train/dummy_d43d28ac.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_bf88f94a.parquet to train/dummy_bf88f94a.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_d843f78d.parquet to train/dummy_d843f78d.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_d8c54683.parquet to train/dummy_d8c54683.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_e2423f6f.parquet to train/dummy_e2423f6f.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_df7ca611.parquet to train/dummy_df7ca611.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_dfc8a691.parquet to train/dummy_dfc8a691.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_e4412086.parquet to train/dummy_e4412086.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_e1e8e502.parquet to train/dummy_e1e8e502.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_e36b4707.parquet to train/dummy_e36b4707.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_ebbfa2a8.parquet to train/dummy_ebbfa2a8.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_ec460023.parquet to train/dummy_ec460023.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_e2ee0d38.parquet to train/dummy_e2ee0d38.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_e6266ac5.parquet to train/dummy_e6266ac5.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_ed5a6ade.parquet to train/dummy_ed5a6ade.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_fce9924e.parquet to train/dummy_fce9924e.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_f8989380.parquet to train/dummy_f8989380.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_f072ba15.parquet to train/dummy_f072ba15.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_edc6a0f9.parquet to train/dummy_edc6a0f9.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_f9c32045.parquet to train/dummy_f9c32045.parquet\n",
      "download: s3://ag-example-timeseries/train/dummy_ecc12c90.parquet to train/dummy_ecc12c90.parquet\n"
     ]
    }
   ],
   "source": [
    "! aws s3 cp s3://{bucket}/train/ train --recursive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "9eb0a4b7-d0e1-425e-8a6a-02bd47d84977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "\n",
    "def _find_parquet_files(root: str, keyword: str | None):\n",
    "    if not root or not os.path.isdir(root):\n",
    "        raise FileNotFoundError(f\"Data directory not found: {root}\")\n",
    "    files = []\n",
    "    for r, _, fns in os.walk(root):\n",
    "        for fn in fns:\n",
    "            _, ext = os.path.splitext(fn)\n",
    "            if ext in ['.parquet']:\n",
    "                if keyword:\n",
    "                    if keyword in fn:\n",
    "                        files.append(os.path.join(r, fn))\n",
    "                else:\n",
    "                    files.append(os.path.join(r, fn))\n",
    "    print(f\"[finder] root={root} keyword={keyword!r} found={len(files)}\")\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No parquet files in {root} (keyword={keyword!r})\")\n",
    "    return sorted(files)\n",
    "\n",
    "def load_timeseries_parquet(\n",
    "    data_root: str,\n",
    "    keyword: str | None,\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    "    target_col: str,\n",
    "    covariate_cols: list[str] | None = None,  # e.g., [\"random_feature\"]\n",
    "):\n",
    "    files = _find_parquet_files(data_root, keyword)\n",
    "\n",
    "    def resolve(cols: list[str], desired: str, aliases: list[str]) -> str | None:\n",
    "        norm = {c.strip().lower(): c for c in cols}\n",
    "        for cand in [desired] + aliases:\n",
    "            k = cand.strip().lower()\n",
    "            if k in norm:\n",
    "                return norm[k]\n",
    "        return None\n",
    "\n",
    "    frames = []\n",
    "    for fp in files:\n",
    "        t = pq.read_table(fp)\n",
    "        df = t.to_pandas()\n",
    "        # print(df)\n",
    "        frames.append(df.reset_index())\n",
    "\n",
    "    all_df = pd.concat(frames, ignore_index=True).sort_values([\"item_id\", \"timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "    target_tsf = TimeSeriesDataFrame.from_data_frame(\n",
    "        all_df[[\"item_id\", \"timestamp\", \"target\"]],\n",
    "        id_column=\"item_id\",\n",
    "        timestamp_column=\"timestamp\",\n",
    "    )\n",
    "\n",
    "    cov_tsf = None\n",
    "    if covariate_cols:\n",
    "        present = [c for c in covariate_cols if c in all_df.columns]  # (only if you merged covs into all_df)\n",
    "        if present:\n",
    "            cov_df = all_df[[\"item_id\", \"timestamp\"] + present]\n",
    "            cov_tsf = TimeSeriesDataFrame.from_data_frame(cov_df, id_column=\"item_id\", timestamp_column=\"timestamp\")\n",
    "\n",
    "    return target_tsf, cov_tsf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "60bf8a7e-20f9-4f3b-ab0a-d1970fa504e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[finder] root=train keyword=None found=100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                  target\n",
       " item_id timestamp                       \n",
       " T000000 2013-03-10 00:00:00  5207.959961\n",
       "         2013-03-10 00:30:00  5002.275879\n",
       "         2013-03-10 01:00:00  4747.569824\n",
       "         2013-03-10 01:30:00  4544.880859\n",
       "         2013-03-10 02:00:00  4425.952148\n",
       " ...                                  ...\n",
       " T000004 2015-02-27 21:30:00   368.948792\n",
       "         2015-02-27 22:00:00   346.332764\n",
       "         2015-02-27 22:30:00   327.962677\n",
       "         2015-02-27 23:00:00   307.481934\n",
       "         2015-02-27 23:30:00   291.532776\n",
       " \n",
       " [172800 rows x 1 columns],\n",
       " None)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_timeseries_parquet('train', None, 'item_id', 'timestamp', 'target')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31321511-788b-4a38-9946-a05431b44d1b",
   "metadata": {},
   "source": [
    "### Create train container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "14fcf0e4-c354-4814-a1de-8d91d22d1abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import os, sys, time, glob, argparse\n",
    "from functools import wraps\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import mlflow\n",
    "\n",
    "from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n",
    "from helper_functions import AGTimeSeriesWrapper  # keep this file in source_dir\n",
    "\n",
    "# ----------------------------\n",
    "# Retry helper\n",
    "# ----------------------------\n",
    "def retry_decorator(max_attempts=3, delay_seconds=60, backoff_factor=2):\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            attempts, delay = 0, delay_seconds\n",
    "            while attempts < max_attempts:\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    attempts += 1\n",
    "                    if attempts >= max_attempts:\n",
    "                        raise\n",
    "                    print(f\"[retry] {e} | attempt {attempts}/{max_attempts} | sleeping {delay}s\")\n",
    "                    time.sleep(delay)\n",
    "                    delay *= backoff_factor\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# ----------------------------\n",
    "# Args\n",
    "# ----------------------------\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument('--output_dir', type=str, default='/opt/ml/model')\n",
    "\n",
    "    # MLflow (managed tracking server ARN, like your tabular script)\n",
    "    p.add_argument('--mlflow_arn', type=str, required=True)\n",
    "    p.add_argument('--mlflow_experiment', type=str, required=True)\n",
    "\n",
    "    # Data roots (default to SageMaker channels)\n",
    "    p.add_argument('--train-dir', type=str, default=os.environ.get('SM_CHANNEL_TRAINING', '/opt/ml/input/data/training'))\n",
    "    p.add_argument('--test-dir',  type=str, default=os.environ.get('SM_CHANNEL_TEST', '/opt/ml/input/data/test'))  # empty means \"no test\"\n",
    "\n",
    "    # Optional filename filters (empty -> load all)\n",
    "    p.add_argument('--train-keyword', type=str, default=None)\n",
    "    p.add_argument('--test-keyword',  type=str, default=None)\n",
    "\n",
    "    # Schema\n",
    "    p.add_argument('--id-col', type=str, default='item_id')\n",
    "    p.add_argument('--time-col', type=str, default='timestamp')\n",
    "    p.add_argument('--target-col', type=str, default='target')\n",
    "\n",
    "    # Model\n",
    "    p.add_argument('--prediction-length', type=int, default=24)\n",
    "    p.add_argument('--eval-metric', type=str, default='MAPE')\n",
    "    p.add_argument('--presets', type=str, default='best_quality')\n",
    "    p.add_argument('--time-limit', type=int, default=900)\n",
    "    p.add_argument('--num-gpus', type=int, default=int(os.environ.get('SM_NUM_GPUS', '0')))\n",
    "    return p.parse_args()\n",
    "\n",
    "# ----------------------------\n",
    "# File discovery (recursive)\n",
    "# ----------------------------\n",
    "PARQUET_EXTS = {\".parquet\", \".PARQUET\", \".pq\", \".PQ\"}\n",
    "\n",
    "def _find_parquet_files(root: str, keyword: str | None):\n",
    "    if not root or not os.path.isdir(root):\n",
    "        raise FileNotFoundError(f\"Data directory not found: {root}\")\n",
    "    files = []\n",
    "    for r, _, fns in os.walk(root):\n",
    "        for fn in fns:\n",
    "            _, ext = os.path.splitext(fn)\n",
    "            if ext in PARQUET_EXTS:\n",
    "                if keyword:\n",
    "                    if keyword in fn:\n",
    "                        files.append(os.path.join(r, fn))\n",
    "                else:\n",
    "                    files.append(os.path.join(r, fn))\n",
    "    print(f\"[finder] root={root} keyword={keyword!r} found={len(files)}\")\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No parquet files in {root} (keyword={keyword!r})\")\n",
    "    return sorted(files)\n",
    "\n",
    "# ----------------------------\n",
    "# Loader -> TSF objects (target + optional covariates)\n",
    "# ----------------------------\n",
    "@retry_decorator(max_attempts=3, delay_seconds=30, backoff_factor=2)\n",
    "def load_timeseries_parquet(\n",
    "    data_root: str,\n",
    "    keyword: str | None,\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    "    target_col: str,\n",
    "    covariate_cols: list[str] | None = None,  # e.g., [\"random_feature\"]\n",
    "):\n",
    "    files = _find_parquet_files(data_root, keyword)\n",
    "\n",
    "    def resolve(cols: list[str], desired: str, aliases: list[str]) -> str | None:\n",
    "        norm = {c.strip().lower(): c for c in cols}\n",
    "        for cand in [desired] + aliases:\n",
    "            k = cand.strip().lower()\n",
    "            if k in norm:\n",
    "                return norm[k]\n",
    "        return None\n",
    "\n",
    "    frames = []\n",
    "    for fp in files:\n",
    "        t = pq.read_table(fp)\n",
    "        df = t.to_pandas()\n",
    "        frames.append(df)\n",
    "\n",
    "    all_df = pd.concat(frames, ignore_index=True).sort_values([\"item_id\", \"timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "    target_tsf = TimeSeriesDataFrame.from_data_frame(\n",
    "        all_df[[\"item_id\", \"timestamp\", \"target\"]],\n",
    "        id_column=\"item_id\",\n",
    "        timestamp_column=\"timestamp\",\n",
    "    )\n",
    "\n",
    "    cov_tsf = None\n",
    "    if covariate_cols:\n",
    "        present = [c for c in covariate_cols if c in all_df.columns]  # (only if you merged covs into all_df)\n",
    "        if present:\n",
    "            cov_df = all_df[[\"item_id\", \"timestamp\"] + present]\n",
    "            cov_tsf = TimeSeriesDataFrame.from_data_frame(cov_df, id_column=\"item_id\", timestamp_column=\"timestamp\")\n",
    "\n",
    "    return target_tsf, cov_tsf\n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    # MLflow (managed server). Requires sagemaker-mlflow installed in the container.\n",
    "    mlflow.set_tracking_uri(args.mlflow_arn)\n",
    "    mlflow.set_experiment(args.mlflow_experiment)\n",
    "\n",
    "    # TRAIN\n",
    "    print(f\"[load] train_dir={args.train_dir} keyword={args.train_keyword!r}\")\n",
    "    train_tsf, train_cov_tsf = load_timeseries_parquet(\n",
    "        args.train_dir, args.train_keyword, args.id_col, args.time_col, args.target_col,\n",
    "        covariate_cols=[\"random_feature\"],\n",
    "    )\n",
    "\n",
    "    # TEST (optional)\n",
    "    train_tsf, train_cov_tsf = load_timeseries_parquet(\n",
    "        args.test_dir, args.test_keyword, args.id_col, args.time_col, args.target_col,\n",
    "        covariate_cols=[\"random_feature\"],\n",
    "    )\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params({\n",
    "            \"prediction_length\": args.prediction_length,\n",
    "            \"eval_metric\": args.eval_metric,\n",
    "            \"presets\": args.presets,\n",
    "            \"time_limit\": args.time_limit,\n",
    "            \"train_dir\": args.train_dir,\n",
    "            \"test_dir\": args.test_dir,\n",
    "            \"train_keyword\": args.train_keyword,\n",
    "            \"test_keyword\": args.test_keyword,\n",
    "        })\n",
    "\n",
    "        predictor = TimeSeriesPredictor(\n",
    "            prediction_length=args.prediction_length,\n",
    "            eval_metric=args.eval_metric,\n",
    "            path=args.output_dir,\n",
    "        )\n",
    "\n",
    "        #TODO add covariates\n",
    "        predictor.fit(\n",
    "            train_data=train_tsf,\n",
    "            presets=args.presets,\n",
    "            time_limit=args.time_limit,\n",
    "        )\n",
    "        predictor.save()\n",
    "\n",
    "        if test_tsf is not None:\n",
    "            scores = predictor.evaluate(test_tsf)\n",
    "            for k, v in scores.items():\n",
    "                mlflow.log_metric(f\"test_{k}\", float(v))\n",
    "\n",
    "         # Register the model with MLflow\n",
    "        run_id = mlflow.last_active_run().info.run_id\n",
    "        artifact_path = \"model\"\n",
    "        model_uri = \"runs:/{run_id}/{artifact_path}\".format(run_id=run_id, artifact_path=artifact_path)\n",
    "        model_details = mlflow.register_model(model_uri=model_uri, name=\"ag_ex_model\")\n",
    "\n",
    "        print(\"[done] training complete and model logged to MLflow.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc46ac60-838b-48c4-a10b-7f3a2f88270b",
   "metadata": {},
   "source": [
    "### Run the training container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "7d390b35-610b-428e-ab6d-e3f9ebef0312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "region      = sagemaker.Session().boto_region_name\n",
    "session     = sagemaker.Session()\n",
    "role        = sagemaker.get_execution_role()  # or set your role arn string\n",
    "\n",
    "instance_type   = \"ml.g5.xlarge\"             # CPU example; use a GPU like \"ml.g5.2xlarge\" if needed\n",
    "instance_count  = 1\n",
    "use_spot        = True                        # optional cost saver\n",
    "max_wait        = 3600 + 600                  # seconds (must be > max_run if use_spot)\n",
    "max_run         = 3600                        # seconds\n",
    "\n",
    "# Hyperparameters for train.py (match argparse names)\n",
    "hyperparameters = {\n",
    "    \"id-col\": \"item_id\",\n",
    "    \"time-col\": \"timestamp\",\n",
    "    \"target-col\": \"target\",\n",
    "    # \"train-keyword\": None,                 # your parquet file name filter\n",
    "    # \"test-keyword\": None,                   # set None/\"\" if no test set\n",
    "    \"prediction-length\": 24,\n",
    "    \"eval-metric\": \"MAPE\",\n",
    "    \"presets\": \"best_quality\",\n",
    "    \"time-limit\": 900,                        # seconds\n",
    "    \"mlflow_arn\": 'arn:aws:sagemaker:us-east-1:543531862107:mlflow-tracking-server/ag-ex-timeseries',\n",
    "    \"mlflow_experiment\": \"autogluon-timeseries\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "2a95a519-93f7-4e5e-853c-8a73e6ce6d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://ag-example-timeseries/train'"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76615857-69b3-4540-b808-9dab6e29223f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training job: ag-ts-train-1757022617-ab4b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: ag-ts-train-1757022617-ab4b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-04 21:50:54 Starting - Starting the training job\n",
      "2025-09-04 21:50:54 Pending - Training job waiting for capacity........................\n",
      "2025-09-04 21:54:53 Pending - Preparing the instances for training...\n",
      "2025-09-04 21:55:18 Downloading - Downloading input data...\n",
      "2025-09-04 21:55:43 Downloading - Downloading the training image.....................\n",
      "2025-09-04 21:59:21 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/paramiko/transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m2025-09-04 21:59:28,207 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2025-09-04 21:59:28,225 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-09-04 21:59:28,235 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2025-09-04 21:59:28,241 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2025-09-04 21:59:31,683 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting mlflow (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading mlflow-3.3.2-py3-none-any.whl.metadata (30 kB)\u001b[0m\n",
      "\u001b[34mCollecting autogluon (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading autogluon-1.4.0-py3-none-any.whl.metadata (11 kB)\u001b[0m\n",
      "\u001b[34mCollecting sagemaker-mlflow (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading sagemaker_mlflow-0.1.0-py3-none-any.whl.metadata (3.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting mlflow-skinny==3.3.2 (from mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading mlflow_skinny-3.3.2-py3-none-any.whl.metadata (31 kB)\u001b[0m\n",
      "\u001b[34mCollecting mlflow-tracing==3.3.2 (from mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading mlflow_tracing-3.3.2-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting Flask<4 (from mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading flask-3.1.2-py3-none-any.whl.metadata (3.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting alembic!=1.10.0,<2 (from mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading alembic-1.16.5-py3-none-any.whl.metadata (7.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cryptography<46,>=43.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow->-r requirements.txt (line 1)) (43.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docker<8,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow->-r requirements.txt (line 1)) (7.1.0)\u001b[0m\n",
      "\u001b[34mCollecting graphene<4 (from mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting gunicorn<24 (from mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib<4 in /opt/conda/lib/python3.10/site-packages (from mlflow->-r requirements.txt (line 1)) (3.9.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy<3 in /opt/conda/lib/python3.10/site-packages (from mlflow->-r requirements.txt (line 1)) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas<3 in /opt/conda/lib/python3.10/site-packages (from mlflow->-r requirements.txt (line 1)) (2.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow<22,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow->-r requirements.txt (line 1)) (17.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn<2 in /opt/conda/lib/python3.10/site-packages (from mlflow->-r requirements.txt (line 1)) (1.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy<2 in /opt/conda/lib/python3.10/site-packages (from mlflow->-r requirements.txt (line 1)) (1.14.0)\u001b[0m\n",
      "\u001b[34mCollecting sqlalchemy<3,>=1.4.0 (from mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading sqlalchemy-2.0.43-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<7,>=5.0.0 (from mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading cachetools-6.2.0-py3-none-any.whl.metadata (5.4 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1)) (8.1.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cloudpickle<4 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1)) (2.2.1)\u001b[0m\n",
      "\u001b[34mCollecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading databricks_sdk-0.65.0-py3-none-any.whl.metadata (39 kB)\u001b[0m\n",
      "\u001b[34mCollecting fastapi<1 (from mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading fastapi-0.116.1-py3-none-any.whl.metadata (28 kB)\u001b[0m\n",
      "\u001b[34mCollecting gitpython<4,>=3.1.9 (from mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1)) (6.11.0)\u001b[0m\n",
      "\u001b[34mCollecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging<26 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<7,>=3.12.0 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1)) (3.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic<3,>=1.10.8 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml<7,>=5.1 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1)) (6.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests<3,>=2.17.3 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1)) (2.32.3)\u001b[0m\n",
      "\u001b[34mCollecting sqlparse<1,>=0.4.0 (from mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading sqlparse-0.5.3-py3-none-any.whl.metadata (3.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions<5,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1)) (4.12.2)\u001b[0m\n",
      "\u001b[34mCollecting uvicorn<1 (from mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting autogluon.core==1.4.0 (from autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading autogluon.core-1.4.0-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting autogluon.features==1.4.0 (from autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading autogluon.features-1.4.0-py3-none-any.whl.metadata (11 kB)\u001b[0m\n",
      "\u001b[34mCollecting autogluon.tabular==1.4.0 (from autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading autogluon.tabular-1.4.0-py3-none-any.whl.metadata (16 kB)\u001b[0m\n",
      "\u001b[34mCollecting autogluon.multimodal==1.4.0 (from autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading autogluon.multimodal-1.4.0-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting autogluon.timeseries==1.4.0 (from autogluon.timeseries[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading autogluon.timeseries-1.4.0-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx<4,>=3.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm<5,>=4.38 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (4.66.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (1.34.158)\u001b[0m\n",
      "\u001b[34mCollecting autogluon.common==1.4.0 (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading autogluon.common-1.4.0-py3-none-any.whl.metadata (11 kB)\u001b[0m\n",
      "\u001b[34mCollecting hyperopt<0.2.8,>=0.2.7 (from autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading hyperopt-0.2.7-py2.py3-none-any.whl.metadata (1.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting ray<2.45,>=2.10.0 (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading ray-2.44.1-cp310-cp310-manylinux2014_x86_64.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow<12,>=10.0.1 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2)) (10.3.0)\u001b[0m\n",
      "\u001b[34mCollecting torch<2.8,>=2.2 (from autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading torch-2.7.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (29 kB)\u001b[0m\n",
      "\u001b[34mCollecting lightning<2.8,>=2.2 (from autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading lightning-2.5.4-py3-none-any.whl.metadata (39 kB)\u001b[0m\n",
      "\u001b[34mCollecting transformers<4.50,>=4.38.0 (from transformers[sentencepiece]<4.50,>=4.38.0->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.0/44.0 kB 6.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting accelerate<2.0,>=0.34.0 (from autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2025.3 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2)) (2024.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema<4.24,>=4.18 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2)) (4.23.0)\u001b[0m\n",
      "\u001b[34mCollecting seqeval<1.3.0,>=1.2.2 (from autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading seqeval-1.2.2.tar.gz (43 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.6/43.6 kB 6.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting evaluate<0.5.0,>=0.4.0 (from autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting timm<1.0.7,>=0.9.5 (from autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading timm-1.0.3-py3-none-any.whl.metadata (43 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.6/43.6 kB 6.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision<0.23.0,>=0.16.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2)) (0.16.0)\u001b[0m\n",
      "\u001b[34mCollecting scikit-image<0.26.0,>=0.19.1 (from autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading scikit_image-0.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\u001b[0m\n",
      "\u001b[34mCollecting text-unidecode<1.4,>=1.3 (from autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting torchmetrics<1.8,>=1.2.0 (from autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading torchmetrics-1.7.4-py3-none-any.whl.metadata (21 kB)\u001b[0m\n",
      "\u001b[34mCollecting omegaconf<2.4.0,>=2.1.1 (from autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting pytorch-metric-learning<2.9,>=1.3.0 (from autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading pytorch_metric_learning-2.8.1-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting nlpaug<1.2.0,>=1.1.10 (from autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\u001b[0m\n",
      "\u001b[34mCollecting nltk<3.10,>=3.4.5 (from autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting openmim<0.4.0,>=0.3.7 (from autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading openmim-0.3.9-py2.py3-none-any.whl.metadata (16 kB)\u001b[0m\n",
      "\u001b[34mCollecting defusedxml<0.7.2,>=0.7.1 (from autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2<3.2,>=3.0.3 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2)) (3.1.4)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard<3,>=2.9 (from autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting pytesseract<0.4,>=0.3.9 (from autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-ml-py3<8.0,>=7.352.0 (from autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting pdf2image<1.19,>=1.17.0 (from autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting catboost<1.3,>=1.2 (from autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading catboost-1.2.8-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fastai<2.9,>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (2.7.16)\u001b[0m\n",
      "\u001b[34mCollecting loguru (from autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\u001b[0m\n",
      "\u001b[34mCollecting lightgbm<4.7,>=4.0 (from autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\u001b[0m\n",
      "\u001b[34mCollecting einx (from autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading einx-0.3.0-py3-none-any.whl.metadata (6.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting xgboost<3.1,>=2.0 (from autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading xgboost-3.0.4-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: spacy<3.9 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (3.7.5)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub[torch] (from autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib<1.7,>=1.2 in /opt/conda/lib/python3.10/site-packages (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (1.4.2)\u001b[0m\n",
      "\u001b[34mCollecting pytorch-lightning (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading pytorch_lightning-2.5.4-py3-none-any.whl.metadata (20 kB)\u001b[0m\n",
      "\u001b[34mCollecting gluonts<0.17,>=0.15.0 (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading gluonts-0.16.2-py3-none-any.whl.metadata (9.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting statsforecast<2.0.2,>=1.7.0 (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading statsforecast-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (29 kB)\u001b[0m\n",
      "\u001b[34mCollecting mlforecast<0.15.0,>=0.14.0 (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading mlforecast-0.14.0-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting utilsforecast<0.2.12,>=0.2.3 (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading utilsforecast-0.2.11-py3-none-any.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting coreforecast<0.0.17,>=0.0.12 (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading coreforecast-0.0.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting fugue>=0.9.0 (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading fugue-0.9.1-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting orjson~=3.9 (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading orjson-3.11.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.9/41.9 kB 5.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil<7.1.0,>=5.7.3 in /opt/conda/lib/python3.10/site-packages (from autogluon.common==1.4.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (6.0.0)\u001b[0m\n",
      "\u001b[34mCollecting Mako (from alembic!=1.10.0,<2->mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tomli in /opt/conda/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow->-r requirements.txt (line 1)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.35.0,>=1.34.158 in /opt/conda/lib/python3.10/site-packages (from boto3<2,>=1.10->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (1.34.158)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2,>=1.10->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2,>=1.10->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (0.10.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography<46,>=43.0.0->mlflow->-r requirements.txt (line 1)) (1.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from docker<8,>=4.0.0->mlflow->-r requirements.txt (line 1)) (1.26.19)\u001b[0m\n",
      "\u001b[34mCollecting blinker>=1.9.0 (from Flask<4->mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting itsdangerous>=2.2.0 (from Flask<4->mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markupsafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow->-r requirements.txt (line 1)) (2.1.5)\u001b[0m\n",
      "\u001b[34mCollecting werkzeug>=3.1.0 (from Flask<4->mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\u001b[0m\n",
      "\u001b[34mCollecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3,>=2.7.0 in /opt/conda/lib/python3.10/site-packages (from graphene<4->mlflow->-r requirements.txt (line 1)) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow->-r requirements.txt (line 1)) (1.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow->-r requirements.txt (line 1)) (0.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow->-r requirements.txt (line 1)) (4.53.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow->-r requirements.txt (line 1)) (1.4.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<4->mlflow->-r requirements.txt (line 1)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3->mlflow->-r requirements.txt (line 1)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<3->mlflow->-r requirements.txt (line 1)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2->mlflow->-r requirements.txt (line 1)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: greenlet>=1 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow->-r requirements.txt (line 1)) (3.0.3)\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.4.3 (from accelerate<2.0,>=0.34.0->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\u001b[0m\n",
      "\u001b[34mCollecting graphviz (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading graphviz-0.21-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (5.23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography<46,>=43.0.0->mlflow->-r requirements.txt (line 1)) (2.21)\u001b[0m\n",
      "\u001b[34mCollecting google-auth~=2.0 (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting datasets>=2.0.0 (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2)) (0.3.8)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2)) (0.70.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (24.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fastdownload<2,>=0.0.5 in /opt/conda/lib/python3.10/site-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (0.0.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fastcore<1.6,>=1.5.29 in /opt/conda/lib/python3.10/site-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (1.5.55)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fastprogress>=0.2.4 in /opt/conda/lib/python3.10/site-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (1.0.3)\u001b[0m\n",
      "\u001b[34mCollecting torch<2.8,>=2.2 (from autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\u001b[0m\n",
      "\u001b[34mCollecting starlette<0.48.0,>=0.40.0 (from fastapi<1->mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading starlette-0.47.3-py3-none-any.whl.metadata (6.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.12.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting triad>=0.9.7 (from fugue>=0.9.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading triad-0.9.8-py3-none-any.whl.metadata (6.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting adagio>=0.2.4 (from fugue>=0.9.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading adagio-0.2.6-py3-none-any.whl.metadata (1.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1 (from gitpython<4,>=3.1.9->mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: toolz~=0.10 in /opt/conda/lib/python3.10/site-packages (from gluonts<0.17,>=0.15.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (0.12.0)\u001b[0m\n",
      "\u001b[34mCollecting future (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting py4j (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1)) (3.19.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2)) (23.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2)) (2023.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2)) (0.35.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2)) (0.20.0)\u001b[0m\n",
      "\u001b[34mCollecting lightning-utilities<2.0,>=0.10.0 (from lightning<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (0.60.0)\u001b[0m\n",
      "\u001b[34mCollecting optuna (from mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\u001b[0m\n",
      "\u001b[34mCollecting window-ops (from mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading window_ops-0.0.15-py3-none-any.whl.metadata (6.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting gdown>=4.0.0 (from nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting regex>=2021.8.3 (from nltk<3.10,>=3.4.5->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading regex-2025.9.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.5/40.5 kB 5.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting antlr4-python3-runtime==4.9.* (from omegaconf<2.4.0,>=2.1.1->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117.0/117.0 kB 17.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2)) (0.4.6)\u001b[0m\n",
      "\u001b[34mCollecting model-index (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading model_index-0.1.11-py3-none-any.whl.metadata (3.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting opendatalab (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading opendatalab-0.0.10-py3-none-any.whl.metadata (6.4 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2)) (13.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2)) (0.9.0)\u001b[0m\n",
      "\u001b[34mCollecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1)) (2.20.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from ray<2.45,>=2.10.0->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (3.15.4)\u001b[0m\n",
      "\u001b[34mCollecting msgpack<2.0.0,>=1.0.0 (from ray<2.45,>=2.10.0->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading msgpack-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal (from ray<2.45,>=2.10.0->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist (from ray<2.45,>=2.10.0->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp_cors (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading aiohttp_cors-0.8.1-py3-none-any.whl.metadata (20 kB)\u001b[0m\n",
      "\u001b[34mCollecting colorful (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading colorful-0.5.7-py2.py3-none-any.whl.metadata (16 kB)\u001b[0m\n",
      "\u001b[34mCollecting py-spy>=0.2.0 (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading py_spy-0.4.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (510 bytes)\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.42.0 (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.74.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting opencensus (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting prometheus_client>=0.7.1 (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading prometheus_client-0.22.1-py3-none-any.whl.metadata (1.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smart_open in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (7.0.4)\u001b[0m\n",
      "\u001b[34mCollecting virtualenv!=20.21.1,>=20.0.24 (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading virtualenv-20.34.0-py3-none-any.whl.metadata (4.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting tensorboardX>=1.9 (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1)) (3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1)) (2024.7.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: imageio!=2.35.0,>=2.33 in /opt/conda/lib/python3.10/site-packages (from scikit-image<0.26.0,>=0.19.1->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2)) (2.34.2)\u001b[0m\n",
      "\u001b[34mCollecting tifffile>=2022.8.12 (from scikit-image<0.26.0,>=0.19.1->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading tifffile-2025.5.10-py3-none-any.whl.metadata (31 kB)\u001b[0m\n",
      "\u001b[34mCollecting lazy-loader>=0.4 (from scikit-image<0.26.0,>=0.19.1->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (3.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (1.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (1.0.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (2.0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (8.2.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (1.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (2.4.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (2.0.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (0.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (0.12.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (72.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (3.4.0)\u001b[0m\n",
      "\u001b[34mCollecting statsmodels>=0.13.2 (from statsforecast<2.0.2,>=1.7.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading statsmodels-0.14.5-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (9.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4 (from tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8 (from tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading markdown-3.9-py3-none-any.whl.metadata (5.1 kB)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2)) (1.13.0)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-curand-cu12==10.3.2.106 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-nccl-cu12==2.20.5 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-nvtx-cu12==12.1.105 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting triton==3.0.0 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.22,>=0.21 (from transformers<4.50,>=4.38.0->transformers[sentencepiece]<4.50,>=4.38.0->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece]<4.50,>=4.38.0->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.2.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting h11>=0.8 (from uvicorn<1->mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting frozendict (from einx->autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading frozendict-2.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\u001b[0m\n",
      "\u001b[34mCollecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub[torch]; extra == \"all\"->autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.6.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.9/73.9 kB 10.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting beautifulsoup4 (from gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading beautifulsoup4-4.13.5-py3-none-any.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<7,>=5.0.0 (from mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1)) (4.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (0.43.0)\u001b[0m\n",
      "\u001b[34mCollecting anyio<5,>=3.6.2 (from starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting patsy>=0.5.6 (from statsmodels>=0.13.2->statsforecast<2.0.2,>=1.7.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading patsy-1.0.1-py2.py3-none-any.whl.metadata (3.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (0.7.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (0.1.4)\u001b[0m\n",
      "\u001b[34mCollecting fs (from triad>=0.9.7->fugue>=0.9.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading fs-2.4.16-py2.py3-none-any.whl.metadata (6.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (1.5.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2)) (2.18.0)\u001b[0m\n",
      "\u001b[34mCollecting distlib<1,>=0.3.7 (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading distlib-0.4.0-py2.py3-none-any.whl.metadata (5.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs<5,>=3.9.1 in /opt/conda/lib/python3.10/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (4.2.2)\u001b[0m\n",
      "\u001b[34mCollecting typing-extensions<5,>=4.0.0 (from mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (0.18.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from smart_open->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting ordered-set (from model-index->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading ordered_set-4.1.0-py3-none-any.whl.metadata (5.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting opencensus-context>=0.1.3 (from opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting google-api-core<3.0.0,>=1.0.0 (from opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading google_api_core-2.25.1-py3-none-any.whl.metadata (3.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting pycryptodome (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting openxlab (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.1.2-py3-none-any.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting colorlog (from optuna->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly->catboost<1.3,>=1.2->autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1)) (1.2.2)\u001b[0m\n",
      "\u001b[34mCollecting sniffio>=1.1 (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting proto-plus<2.0.0,>=1.22.3 (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: marisa-trie>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon->-r requirements.txt (line 2)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2)) (0.1.2)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting soupsieve>1.2 (from beautifulsoup4->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting appdirs~=1.4.3 (from fs->triad>=0.9.7->fugue>=0.9.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting filelock (from ray<2.45,>=2.10.0->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting oss2~=2.17.0 (from openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading oss2-2.17.0.tar.gz (259 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 259.5/259.5 kB 26.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting packaging<26 (from mlflow-skinny==3.3.2->mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting pytz>=2020.1 (from pandas<3->mlflow->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading pytz-2023.4-py2.py3-none-any.whl.metadata (22 kB)\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of openxlab to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mCollecting openxlab (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.1.1-py3-none-any.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.1.0-py3-none-any.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.0.38-py3-none-any.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.0.37-py3-none-any.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.0.36-py3-none-any.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.0.35-py3-none-any.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.0.34-py3-none-any.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mINFO: pip is still looking at multiple versions of openxlab to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.0.33-py3-none-any.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.0.32-py3-none-any.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.0.31-py3-none-any.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.0.30-py3-none-any.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.0.29-py3-none-any.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.0.28-py3-none-any.whl.metadata (3.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.0.27-py3-none-any.whl.metadata (3.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.0.26-py3-none-any.whl.metadata (3.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.0.25-py3-none-any.whl.metadata (3.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.0.24-py3-none-any.whl.metadata (3.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.0.23-py3-none-any.whl.metadata (3.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.0.22-py3-none-any.whl.metadata (3.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.0.21-py3-none-any.whl.metadata (3.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.0.20-py3-none-any.whl.metadata (3.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.0.19-py3-none-any.whl.metadata (3.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.0.18-py3-none-any.whl.metadata (3.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.0.17-py3-none-any.whl.metadata (3.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.0.16-py3-none-any.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.0.15-py3-none-any.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.0.14-py3-none-any.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.0.13-py3-none-any.whl.metadata (4.5 kB)\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.0.12-py3-none-any.whl.metadata (4.5 kB)\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.0.11-py3-none-any.whl.metadata (4.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.4.0->autogluon->-r requirements.txt (line 2)) (1.7.1)\u001b[0m\n",
      "\u001b[34mDownloading mlflow-3.3.2-py3-none-any.whl (26.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.4/26.4 MB 68.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading mlflow_skinny-3.3.2-py3-none-any.whl (2.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 77.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading mlflow_tracing-3.3.2-py3-none-any.whl (1.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 103.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading autogluon-1.4.0-py3-none-any.whl (9.8 kB)\u001b[0m\n",
      "\u001b[34mDownloading autogluon.core-1.4.0-py3-none-any.whl (225 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 225.1/225.1 kB 46.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading autogluon.features-1.4.0-py3-none-any.whl (64 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.2/64.2 kB 18.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading autogluon.multimodal-1.4.0-py3-none-any.whl (454 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 454.9/454.9 kB 72.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading autogluon.tabular-1.4.0-py3-none-any.whl (487 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 487.3/487.3 kB 80.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading autogluon.timeseries-1.4.0-py3-none-any.whl (189 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 189.7/189.7 kB 41.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading autogluon.common-1.4.0-py3-none-any.whl (70 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.0/71.0 kB 21.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading sagemaker_mlflow-0.1.0-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mDownloading alembic-1.16.5-py3-none-any.whl (247 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.4/247.4 kB 53.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading flask-3.1.2-py3-none-any.whl (103 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103.3/103.3 kB 28.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.9/114.9 kB 30.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.0/85.0 kB 24.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading sqlalchemy-2.0.43-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.2/3.2 MB 119.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading accelerate-1.10.1-py3-none-any.whl (374 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 374.9/374.9 kB 65.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\u001b[0m\n",
      "\u001b[34mDownloading catboost-1.2.8-cp310-cp310-manylinux2014_x86_64.whl (99.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 99.2/99.2 MB 39.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading coreforecast-0.0.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 284.0/284.0 kB 55.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading databricks_sdk-0.65.0-py3-none-any.whl (705 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 705.9/705.9 kB 90.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.5-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.1/84.1 kB 24.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading fastapi-0.116.1-py3-none-any.whl (95 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 95.6/95.6 kB 26.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading fugue-0.9.1-py3-none-any.whl (278 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 278.2/278.2 kB 53.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading gitpython-3.1.45-py3-none-any.whl (208 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 208.2/208.2 kB 46.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading gluonts-0.16.2-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 80.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 203.4/203.4 kB 45.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[34mDownloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 63.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[34mDownloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 76.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading lightning-2.5.4-py3-none-any.whl (825 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 825.2/825.2 kB 65.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading mlforecast-0.14.0-py3-none-any.whl (71 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.0/72.0 kB 20.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.5/410.5 kB 71.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 111.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.5/79.5 kB 22.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading openmim-0.3.9-py2.py3-none-any.whl (52 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.7/52.7 kB 13.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.6/65.6 kB 18.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120.0/120.0 kB 31.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 201.6/201.6 kB 48.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading orjson-3.11.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.0/133.0 kB 35.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\u001b[0m\n",
      "\u001b[34mDownloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mDownloading pytorch_metric_learning-2.8.1-py3-none-any.whl (125 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 125.9/125.9 kB 33.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading ray-2.44.1-cp310-cp310-manylinux2014_x86_64.whl (67.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.9/67.9 MB 51.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading scikit_image-0.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.8/14.8 MB 96.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading sqlparse-0.5.3-py3-none-any.whl (44 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.4/44.4 kB 11.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading statsforecast-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (353 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 353.6/353.6 kB 58.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 117.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.2/78.2 kB 23.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading timm-1.0.3-py3-none-any.whl (2.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 111.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl (797.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 797.1/797.1 MB 1.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.6/410.6 MB 4.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/14.1 MB 116.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.7/23.7 MB 41.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 823.6/823.6 kB 90.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 664.8/664.8 MB 2.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.6/121.6 MB 31.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.5/56.5 MB 58.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.2/124.2 MB 34.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 196.0/196.0 MB 14.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 176.2/176.2 MB 18.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 99.1/99.1 kB 24.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 209.4/209.4 MB 12.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading torchmetrics-1.7.4-py3-none-any.whl (963 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 963.5/963.5 kB 93.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.0/10.0 MB 133.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading utilsforecast-0.2.11-py3-none-any.whl (41 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.7/41.7 kB 12.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.4/66.4 kB 20.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 45.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading xgboost-3.0.4-py3-none-manylinux_2_28_x86_64.whl (94.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94.9/94.9 MB 38.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading einx-0.3.0-py3-none-any.whl (102 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103.0/103.0 kB 27.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.6/61.6 kB 18.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading mako-1.3.10-py3-none-any.whl (78 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.5/78.5 kB 23.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pytorch_lightning-2.5.4-py3-none-any.whl (829 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 829.2/829.2 kB 64.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading absl_py-2.3.1-py3-none-any.whl (135 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.8/135.8 kB 32.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading adagio-0.2.6-py3-none-any.whl (19 kB)\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.12.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 106.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\u001b[0m\n",
      "\u001b[34mDownloading datasets-4.0.0-py3-none-any.whl (494 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 494.8/494.8 kB 60.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.7.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 222.9/222.9 kB 39.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.12-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 kB 18.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 216.1/216.1 kB 45.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading cachetools-5.5.2-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.74.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.2/6.2 MB 112.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading h11-0.16.0-py3-none-any.whl (37 kB)\u001b[0m\n",
      "\u001b[34mDownloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.2/3.2 MB 116.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 561.5/561.5 kB 81.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading lazy_loader-0.4-py3-none-any.whl (12 kB)\u001b[0m\n",
      "\u001b[34mDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\u001b[0m\n",
      "\u001b[34mDownloading markdown-3.9-py3-none-any.whl (107 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 107.4/107.4 kB 27.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading msgpack-1.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (408 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 408.6/408.6 kB 70.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading prometheus_client-0.22.1-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.7/58.7 kB 17.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading py_spy-0.4.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.8/2.8 MB 63.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading regex-2025.9.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (789 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 789.9/789.9 kB 44.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 485.8/485.8 kB 35.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.2.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 52.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading starlette-0.47.3-py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.0/73.0 kB 21.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading statsmodels-0.14.5-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (10.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.7/10.7 MB 123.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 117.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.2/87.2 kB 23.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tifffile-2025.5.10-py3-none-any.whl (226 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 226.5/226.5 kB 50.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 126.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading triad-0.9.8-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.3/62.3 kB 19.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading virtualenv-20.34.0-py3-none-any.whl (6.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.0/6.0 MB 134.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.6/44.6 kB 13.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiohttp_cors-0.8.1-py3-none-any.whl (25 kB)\u001b[0m\n",
      "\u001b[34mDownloading colorful-0.5.7-py2.py3-none-any.whl (201 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 201.5/201.5 kB 43.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading frozendict-2.4.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117.4/117.4 kB 29.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading future-1.0.0-py3-none-any.whl (491 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 491.3/491.3 kB 76.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading graphviz-0.21-py3-none-any.whl (47 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.3/47.3 kB 14.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading model_index-0.1.11-py3-none-any.whl (34 kB)\u001b[0m\n",
      "\u001b[34mDownloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.2/128.2 kB 33.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading opendatalab-0.0.10-py3-none-any.whl (29 kB)\u001b[0m\n",
      "\u001b[34mDownloading optuna-4.5.0-py3-none-any.whl (400 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 400.9/400.9 kB 67.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 203.0/203.0 kB 45.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading window_ops-0.0.15-py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 39.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mDownloading anyio-4.10.0-py3-none-any.whl (107 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 107.2/107.2 kB 28.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\u001b[0m\n",
      "\u001b[34mDownloading distlib-0.4.0-py2.py3-none-any.whl (469 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 469.0/469.0 kB 77.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading google_api_core-2.25.1-py3-none-any.whl (160 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 160.8/160.8 kB 40.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.6.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 241.6/241.6 kB 53.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\u001b[0m\n",
      "\u001b[34mDownloading patsy-1.0.1-py2.py3-none-any.whl (232 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 232.9/232.9 kB 49.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading propcache-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 198.3/198.3 kB 44.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.3/181.3 kB 43.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.2-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 326.1/326.1 kB 59.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading beautifulsoup4-4.13.5-py3-none-any.whl (105 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105.1/105.1 kB 28.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\u001b[0m\n",
      "\u001b[34mDownloading fs-2.4.16-py2.py3-none-any.whl (135 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.3/135.3 kB 34.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 39.7/39.7 MB 74.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading openxlab-0.0.11-py3-none-any.whl (55 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.3/55.3 kB 15.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mDownloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 104.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\u001b[0m\n",
      "\u001b[34mDownloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 294.5/294.5 kB 59.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.2/50.2 kB 14.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.1/83.1 kB 23.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mDownloading soupsieve-2.8-py3-none-any.whl (36 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: nvidia-ml-py3, antlr4-python3-runtime, seqeval\u001b[0m\n",
      "\u001b[34mBuilding wheel for nvidia-ml-py3 (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for nvidia-ml-py3 (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19173 sha256=cfa9499515643bdbe2b93386b08ec3d3098d4d7a363868e8880169b9f8d50ec4\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/5c/d8/c0/46899f8be7a75a2ffd197a23c8797700ea858b9b34819fbf9e\u001b[0m\n",
      "\u001b[34mBuilding wheel for antlr4-python3-runtime (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144552 sha256=28affb3e96d79f452d9796d0ed2f6e0937c323e8be2050deba7eee2895399cd9\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\u001b[0m\n",
      "\u001b[34mBuilding wheel for seqeval (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for seqeval (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=7a354dba3bd7a148e778f7ad5d649585cacbe3c49587c27f4d596bafff3470ed\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\u001b[0m\n",
      "\u001b[34mSuccessfully built nvidia-ml-py3 antlr4-python3-runtime seqeval\u001b[0m\n",
      "\u001b[34mInstalling collected packages: text-unidecode, py4j, py-spy, opencensus-context, nvidia-ml-py3, distlib, colorful, appdirs, antlr4-python3-runtime, xxhash, werkzeug, typing-extensions, triton, tifffile, tensorboardX, tensorboard-data-server, sqlparse, soupsieve, sniffio, smmap, sentencepiece, safetensors, regex, pytesseract, pycryptodome, pyasn1, proto-plus, propcache, prometheus_client, pdf2image, patsy, orjson, ordered-set, openxlab, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgpack, markdown, Mako, loguru, lazy-loader, itsdangerous, hf-xet, h11, gunicorn, grpcio, graphviz, graphql-core, googleapis-common-protos, future, fs, frozenlist, frozendict, defusedxml, coreforecast, colorlog, cachetools, blinker, async-timeout, aiohappyeyeballs, absl-py, xgboost, window-ops, virtualenv, uvicorn, tensorboard, sqlalchemy, scikit-image, pyasn1-modules, opentelemetry-api, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nltk, multidict, model-index, lightning-utilities, lightgbm, hyperopt, huggingface-hub, graphql-relay, gitdb, Flask, einx, beautifulsoup4, anyio, aiosignal, yarl, utilsforecast, triad, tokenizers, statsmodels, starlette, seqeval, opentelemetry-semantic-conventions, opendatalab, nvidia-cusolver-cu12, graphene, google-auth, gitpython, gdown, catboost, alembic, transformers, torch, ray, optuna, opentelemetry-sdk, openmim, nlpaug, google-api-core, gluonts, fastapi, databricks-sdk, aiohttp, adagio, torchmetrics, pytorch-metric-learning, opencensus, mlforecast, mlflow-tracing, mlflow-skinny, fugue, autogluon.common, aiohttp_cors, accelerate, timm, statsforecast, pytorch-lightning, mlflow, datasets, autogluon.features, autogluon.core, sagemaker-mlflow, lightning, evaluate, autogluon.tabular, autogluon.multimodal, autogluon.timeseries, autogluon\u001b[0m\n",
      "\u001b[34mAttempting uninstall: werkzeug\u001b[0m\n",
      "\u001b[34mFound existing installation: Werkzeug 3.0.3\u001b[0m\n",
      "\u001b[34mUninstalling Werkzeug-3.0.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled Werkzeug-3.0.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: typing-extensions\u001b[0m\n",
      "\u001b[34mFound existing installation: typing_extensions 4.12.2\u001b[0m\n",
      "\u001b[34mUninstalling typing_extensions-4.12.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled typing_extensions-4.12.2\u001b[0m\n",
      "\u001b[34mAttempting uninstall: triton\u001b[0m\n",
      "\u001b[34mFound existing installation: triton 2.1.0\u001b[0m\n",
      "\u001b[34mUninstalling triton-2.1.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled triton-2.1.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: pyasn1\u001b[0m\n",
      "\u001b[34mFound existing installation: pyasn1 0.6.0\u001b[0m\n",
      "\u001b[34mUninstalling pyasn1-0.6.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled pyasn1-0.6.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 2.1.0\u001b[0m\n",
      "\u001b[34mUninstalling torch-2.1.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-2.1.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.22.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.22.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.22.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed Flask-3.1.2 Mako-1.3.10 absl-py-2.3.1 accelerate-1.10.1 adagio-0.2.6 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiohttp_cors-0.8.1 aiosignal-1.4.0 alembic-1.16.5 antlr4-python3-runtime-4.9.3 anyio-4.10.0 appdirs-1.4.4 async-timeout-5.0.1 autogluon-1.4.0 autogluon.common-1.4.0 autogluon.core-1.4.0 autogluon.features-1.4.0 autogluon.multimodal-1.4.0 autogluon.tabular-1.4.0 autogluon.timeseries-1.4.0 beautifulsoup4-4.13.5 blinker-1.9.0 cachetools-5.5.2 catboost-1.2.8 colorful-0.5.7 colorlog-6.9.0 coreforecast-0.0.16 databricks-sdk-0.65.0 datasets-4.0.0 defusedxml-0.7.1 distlib-0.4.0 einx-0.3.0 evaluate-0.4.5 fastapi-0.116.1 frozendict-2.4.6 frozenlist-1.7.0 fs-2.4.16 fugue-0.9.1 future-1.0.0 gdown-5.2.0 gitdb-4.0.12 gitpython-3.1.45 gluonts-0.16.2 google-api-core-2.25.1 google-auth-2.40.3 googleapis-common-protos-1.70.0 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 graphviz-0.21 grpcio-1.74.0 gunicorn-23.0.0 h11-0.16.0 hf-xet-1.1.9 huggingface-hub-0.34.4 hyperopt-0.2.7 itsdangerous-2.2.0 lazy-loader-0.4 lightgbm-4.6.0 lightning-2.5.4 lightning-utilities-0.15.2 loguru-0.7.3 markdown-3.9 mlflow-3.3.2 mlflow-skinny-3.3.2 mlflow-tracing-3.3.2 mlforecast-0.14.0 model-index-0.1.11 msgpack-1.1.1 multidict-6.6.4 nlpaug-1.1.11 nltk-3.9.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-ml-py3-7.352.0 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 omegaconf-2.3.0 opencensus-0.11.4 opencensus-context-0.1.3 opendatalab-0.0.10 openmim-0.3.9 opentelemetry-api-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 openxlab-0.0.11 optuna-4.5.0 ordered-set-4.1.0 orjson-3.11.3 patsy-1.0.1 pdf2image-1.17.0 prometheus_client-0.22.1 propcache-0.3.2 proto-plus-1.26.1 py-spy-0.4.1 py4j-0.10.9.9 pyasn1-0.6.1 pyasn1-modules-0.4.2 pycryptodome-3.23.0 pytesseract-0.3.13 pytorch-lightning-2.5.4 pytorch-metric-learning-2.8.1 ray-2.44.1 regex-2025.9.1 safetensors-0.6.2 sagemaker-mlflow-0.1.0 scikit-image-0.25.2 sentencepiece-0.2.1 seqeval-1.2.2 smmap-5.0.2 sniffio-1.3.1 soupsieve-2.8 sqlalchemy-2.0.43 sqlparse-0.5.3 starlette-0.47.3 statsforecast-2.0.1 statsmodels-0.14.5 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorboardX-2.6.4 text-unidecode-1.3 tifffile-2025.5.10 timm-1.0.3 tokenizers-0.21.4 torch-2.4.1 torchmetrics-1.7.4 transformers-4.49.0 triad-0.9.8 triton-3.0.0 typing-extensions-4.15.0 utilsforecast-0.2.11 uvicorn-0.35.0 virtualenv-20.34.0 werkzeug-3.1.3 window-ops-0.0.15 xgboost-3.0.4 xxhash-3.5.0 yarl-1.20.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 24.1.2 -> 25.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2025-09-04 22:02:33,573 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2025-09-04 22:02:33,573 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2025-09-04 22:02:33,598 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-09-04 22:02:33,632 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-09-04 22:02:33,724 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-09-04 22:02:33,735 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"eval-metric\": \"MAPE\",\n",
      "        \"id-col\": \"item_id\",\n",
      "        \"mlflow_arn\": \"arn:aws:sagemaker:us-east-1:543531862107:mlflow-tracking-server/ag-ex-timeseries\",\n",
      "        \"mlflow_experiment\": \"autogluon-timeseries\",\n",
      "        \"prediction-length\": 24,\n",
      "        \"presets\": \"best_quality\",\n",
      "        \"target-col\": \"target\",\n",
      "        \"time-col\": \"timestamp\",\n",
      "        \"time-limit\": 900\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"ContentType\": \"application/x-parquet\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"ContentType\": \"application/x-parquet\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"ag-ts-train-1757022617-ab4b\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-543531862107/ag-ts-train-1757022617-ab4b/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"topology\": null\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"eval-metric\":\"MAPE\",\"id-col\":\"item_id\",\"mlflow_arn\":\"arn:aws:sagemaker:us-east-1:543531862107:mlflow-tracking-server/ag-ex-timeseries\",\"mlflow_experiment\":\"autogluon-timeseries\",\"prediction-length\":24,\"presets\":\"best_quality\",\"target-col\":\"target\",\"time-col\":\"timestamp\",\"time-limit\":900}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"ContentType\":\"application/x-parquet\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"ContentType\":\"application/x-parquet\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-543531862107/ag-ts-train-1757022617-ab4b/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"eval-metric\":\"MAPE\",\"id-col\":\"item_id\",\"mlflow_arn\":\"arn:aws:sagemaker:us-east-1:543531862107:mlflow-tracking-server/ag-ex-timeseries\",\"mlflow_experiment\":\"autogluon-timeseries\",\"prediction-length\":24,\"presets\":\"best_quality\",\"target-col\":\"target\",\"time-col\":\"timestamp\",\"time-limit\":900},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"ContentType\":\"application/x-parquet\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"ContentType\":\"application/x-parquet\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"ag-ts-train-1757022617-ab4b\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-543531862107/ag-ts-train-1757022617-ab4b/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--eval-metric\",\"MAPE\",\"--id-col\",\"item_id\",\"--mlflow_arn\",\"arn:aws:sagemaker:us-east-1:543531862107:mlflow-tracking-server/ag-ex-timeseries\",\"--mlflow_experiment\",\"autogluon-timeseries\",\"--prediction-length\",\"24\",\"--presets\",\"best_quality\",\"--target-col\",\"target\",\"--time-col\",\"timestamp\",\"--time-limit\",\"900\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL-METRIC=MAPE\u001b[0m\n",
      "\u001b[34mSM_HP_ID-COL=item_id\u001b[0m\n",
      "\u001b[34mSM_HP_MLFLOW_ARN=arn:aws:sagemaker:us-east-1:543531862107:mlflow-tracking-server/ag-ex-timeseries\u001b[0m\n",
      "\u001b[34mSM_HP_MLFLOW_EXPERIMENT=autogluon-timeseries\u001b[0m\n",
      "\u001b[34mSM_HP_PREDICTION-LENGTH=24\u001b[0m\n",
      "\u001b[34mSM_HP_PRESETS=best_quality\u001b[0m\n",
      "\u001b[34mSM_HP_TARGET-COL=target\u001b[0m\n",
      "\u001b[34mSM_HP_TIME-COL=timestamp\u001b[0m\n",
      "\u001b[34mSM_HP_TIME-LIMIT=900\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train.py --eval-metric MAPE --id-col item_id --mlflow_arn arn:aws:sagemaker:us-east-1:543531862107:mlflow-tracking-server/ag-ex-timeseries --mlflow_experiment autogluon-timeseries --prediction-length 24 --presets best_quality --target-col target --time-col timestamp --time-limit 900\u001b[0m\n",
      "\u001b[34m2025-09-04 22:02:33,737 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2025-09-04 22:02:33,737 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m[load] train_dir=/opt/ml/input/data/training keyword=None\u001b[0m\n",
      "\u001b[34m[finder] root=/opt/ml/input/data/training keyword=None found=100\u001b[0m\n",
      "\u001b[34m[finder] root=/opt/ml/input/data/test keyword=None found=100\u001b[0m\n",
      "\u001b[34mWarning: path already exists! This predictor may overwrite an existing predictor! path=\"/opt/ml/model\"\u001b[0m\n",
      "\u001b[34mBeginning AutoGluon training... Time limit = 900s\u001b[0m\n",
      "\u001b[34mAutoGluon will save models to '/opt/ml/model'\u001b[0m\n",
      "\u001b[34m=================== System Info ===================\u001b[0m\n",
      "\u001b[34mAutoGluon Version:  1.4.0\u001b[0m\n",
      "\u001b[34mPython Version:     3.10.12\u001b[0m\n",
      "\u001b[34mOperating System:   Linux\u001b[0m\n",
      "\u001b[34mPlatform Machine:   x86_64\u001b[0m\n",
      "\u001b[34mPlatform Version:   #1 SMP Tue Jul 29 04:38:43 UTC 2025\u001b[0m\n",
      "\u001b[34mCPU Count:          4\u001b[0m\n",
      "\u001b[34mGPU Count:          1\u001b[0m\n",
      "\u001b[34mMemory Avail:       13.53 GB / 15.43 GB (87.7%)\u001b[0m\n",
      "\u001b[34mDisk Space Avail:   217.32 GB / 228.99 GB (94.9%)\u001b[0m\n",
      "\u001b[34m===================================================\u001b[0m\n",
      "\u001b[34mSetting presets to: best_quality\u001b[0m\n",
      "\u001b[34mFitting with arguments:\u001b[0m\n",
      "\u001b[34m{'enable_ensemble': True,\n",
      " 'eval_metric': MAPE,\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 2,\n",
      " 'prediction_length': 24,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'target',\n",
      " 'time_limit': 900,\n",
      " 'verbosity': 2}\u001b[0m\n",
      "\u001b[34mInferred time series frequency: '30min'\u001b[0m\n",
      "\u001b[34mProvided train_data has 173040 rows, 5 time series. Median time series length is 34608 (min=34608, max=34608).\u001b[0m\n",
      "\u001b[34mProvided data contains following columns:\u001b[0m\n",
      "\u001b[34mtarget: 'target'\u001b[0m\n",
      "\u001b[34mAutoGluon will gauge predictive performance using evaluation metric: 'MAPE'\u001b[0m\n",
      "\u001b[34m#011This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\u001b[0m\n",
      "\u001b[34m===================================================\u001b[0m\n",
      "\u001b[34mStarting training. Start time is 2025-09-04 22:02:40\u001b[0m\n",
      "\u001b[34mModels that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'NPTS', 'DynamicOptimizedTheta', 'AutoETS', 'ChronosZeroShot[bolt_base]', 'ChronosFineTuned[bolt_small]', 'TemporalFusionTransformer', 'DeepAR', 'PatchTST', 'TiDE']\u001b[0m\n",
      "\u001b[34mTraining timeseries model SeasonalNaive. Training for up to 69.0s of the 897.2s of remaining time.\u001b[0m\n",
      "\u001b[34m-0.1381       = Validation score (-MAPE)\u001b[0m\n",
      "\u001b[34m1.31    s     = Training runtime\u001b[0m\n",
      "\u001b[34m0.02    s     = Validation (prediction) runtime\u001b[0m\n",
      "\u001b[34mTraining timeseries model RecursiveTabular. Training for up to 74.7s of the 895.9s of remaining time.\u001b[0m\n",
      "\u001b[34m-0.0937       = Validation score (-MAPE)\u001b[0m\n",
      "\u001b[34m29.71   s     = Training runtime\u001b[0m\n",
      "\u001b[34m0.18    s     = Validation (prediction) runtime\u001b[0m\n",
      "\u001b[34mTraining timeseries model DirectTabular. Training for up to 78.7s of the 865.9s of remaining time.\u001b[0m\n",
      "\u001b[34m-0.1051       = Validation score (-MAPE)\u001b[0m\n",
      "\u001b[34m36.90   s     = Training runtime\u001b[0m\n",
      "\u001b[34m0.44    s     = Validation (prediction) runtime\u001b[0m\n",
      "\u001b[34mTraining timeseries model NPTS. Training for up to 82.8s of the 828.4s of remaining time.\u001b[0m\n",
      "\u001b[34m-0.1793       = Validation score (-MAPE)\u001b[0m\n",
      "\u001b[34m1.04    s     = Training runtime\u001b[0m\n",
      "\u001b[34m#0110.99    s     = Validation (prediction) runtime\u001b[0m\n",
      "\u001b[34mTraining timeseries model DynamicOptimizedTheta. Training for up to 91.8s of the 826.3s of remaining time.\u001b[0m\n",
      "\u001b[34m-0.0798       = Validation score (-MAPE)\u001b[0m\n",
      "\u001b[34m1.23    s     = Training runtime\u001b[0m\n",
      "\u001b[34m#0110.68    s     = Validation (prediction) runtime\u001b[0m\n",
      "\u001b[34mTraining timeseries model AutoETS. Training for up to 103.1s of the 824.4s of remaining time.\u001b[0m\n",
      "\u001b[34m-0.1559       = Validation score (-MAPE)\u001b[0m\n",
      "\u001b[34m4.49    s     = Training runtime\u001b[0m\n",
      "\u001b[34m#0114.49    s     = Validation (prediction) runtime\u001b[0m\n",
      "\u001b[34mTraining timeseries model ChronosZeroShot[bolt_base]. Training for up to 116.5s of the 815.4s of remaining time.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\u001b[0m\n",
      "\u001b[34mWarning: Exception caused ChronosZeroShot[bolt_base] to fail during training... Skipping this model.\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/timeseries/trainer.py\", line 357, in _train_and_save\n",
      "    model = self._train_single(train_data, model, val_data=val_data, time_limit=time_limit)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/timeseries/trainer.py\", line 273, in _train_single\n",
      "    model.fit(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/timeseries/models/abstract/abstract_timeseries_model.py\", line 515, in fit\n",
      "    self._fit(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/timeseries/models/multi_window/multi_window_model.py\", line 151, in _fit\n",
      "    model.score_and_cache_oof(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/timeseries/models/abstract/abstract_timeseries_model.py\", line 730, in score_and_cache_oof\n",
      "    oof_predictions = self.predict(past_data, known_covariates=known_covariates, **predict_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/timeseries/models/abstract/abstract_timeseries_model.py\", line 608, in predict\n",
      "    predictions = self._predict(data=data, known_covariates=known_covariates, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/timeseries/models/chronos/model.py\", line 637, in _predict\n",
      "    self.model_pipeline.model.eval()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/timeseries/models/chronos/model.py\", line 240, in model_pipeline\n",
      "    self.load_model_pipeline()  # load model pipeline to device memory\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/timeseries/models/chronos/model.py\", line 295, in load_model_pipeline\n",
      "    pipeline = BaseChronosPipeline.from_pretrained(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/timeseries/models/chronos/pipeline/base.py\", line 160, in from_pretrained\n",
      "    return class_.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/timeseries/models/chronos/pipeline/chronos_bolt.py\", line 529, in from_pretrained\n",
      "    model = class_.from_pretrained(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 262, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 4185, in from_pretrained\n",
      "    model = cls(config, *model_args, **model_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/timeseries/models/chronos/pipeline/chronos_bolt.py\", line 179, in __init__\n",
      "    self.encoder = T5Stack(encoder_config, self.shared)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 889, in __init__\n",
      "    [T5Block(config, has_relative_attention_bias=bool(i == 0), layer_idx=i) for i in range(config.num_layers)]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 889, in <listcomp>\n",
      "    [T5Block(config, has_relative_attention_bias=bool(i == 0), layer_idx=i) for i in range(config.num_layers)]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 652, in __init__\n",
      "    T5LayerSelfAttention(config, has_relative_attention_bias=has_relative_attention_bias, layer_idx=layer_idx)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 578, in __init__\n",
      "    self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/apex/normalization/fused_layer_norm.py\", line 364, in __init__\n",
      "    fused_layer_norm_cuda = importlib.import_module(\"fused_layer_norm_cuda\")\n",
      "  File \"/opt/conda/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 674, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1176, in create_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34mImportError: /opt/conda/lib/python3.10/site-packages/fused_layer_norm_cuda.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefINS2_6SymIntEEENS2_8optionalINS2_10ScalarTypeEEENS6_INS2_6LayoutEEENS6_INS2_6DeviceEEENS6_IbEENS6_INS2_12MemoryFormatEEE\u001b[0m\n",
      "\u001b[34mTraining timeseries model ChronosFineTuned[bolt_small]. Training for up to 135.1s of the 810.7s of remaining time.\u001b[0m\n",
      "\u001b[34mSkipping covariate_regressor since the dataset contains no known_covariates or static_features.\u001b[0m\n",
      "\u001b[34mWarning: Exception caused ChronosFineTuned[bolt_small] to fail during training... Skipping this model.\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/timeseries/trainer.py\", line 357, in _train_and_save\n",
      "    model = self._train_single(train_data, model, val_data=val_data, time_limit=time_limit)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/timeseries/trainer.py\", line 273, in _train_single\n",
      "    model.fit(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/timeseries/models/abstract/abstract_timeseries_model.py\", line 515, in fit\n",
      "    self._fit(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/timeseries/models/multi_window/multi_window_model.py\", line 137, in _fit\n",
      "    model.fit(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/timeseries/models/abstract/abstract_timeseries_model.py\", line 515, in fit\n",
      "    self._fit(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/timeseries/models/chronos/model.py\", line 459, in _fit\n",
      "    self.load_model_pipeline(is_training=True)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/timeseries/models/chronos/model.py\", line 295, in load_model_pipeline\n",
      "    pipeline = BaseChronosPipeline.from_pretrained(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/timeseries/models/chronos/pipeline/base.py\", line 160, in from_pretrained\n",
      "    return class_.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/timeseries/models/chronos/pipeline/chronos_bolt.py\", line 529, in from_pretrained\n",
      "    model = class_.from_pretrained(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 262, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 4185, in from_pretrained\n",
      "    model = cls(config, *model_args, **model_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/autogluon/timeseries/models/chronos/pipeline/chronos_bolt.py\", line 179, in __init__\n",
      "    self.encoder = T5Stack(encoder_config, self.shared)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 889, in __init__\n",
      "    [T5Block(config, has_relative_attention_bias=bool(i == 0), layer_idx=i) for i in range(config.num_layers)]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 889, in <listcomp>\n",
      "    [T5Block(config, has_relative_attention_bias=bool(i == 0), layer_idx=i) for i in range(config.num_layers)]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 652, in __init__\n",
      "    T5LayerSelfAttention(config, has_relative_attention_bias=has_relative_attention_bias, layer_idx=layer_idx)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py\", line 578, in __init__\n",
      "    self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/apex/normalization/fused_layer_norm.py\", line 364, in __init__\n",
      "    fused_layer_norm_cuda = importlib.import_module(\"fused_layer_norm_cuda\")\n",
      "  File \"/opt/conda/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 674, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 571, in module_from_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1176, in create_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34mImportError: /opt/conda/lib/python3.10/site-packages/fused_layer_norm_cuda.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN2at4_ops19empty_memory_format4callEN3c108ArrayRefINS2_6SymIntEEENS2_8optionalINS2_10ScalarTypeEEENS6_INS2_6LayoutEEENS6_INS2_6DeviceEEENS6_IbEENS6_INS2_12MemoryFormatEEE\u001b[0m\n",
      "\u001b[34mTraining timeseries model TemporalFusionTransformer. Training for up to 161.9s of the 809.7s of remaining time.\u001b[0m\n",
      "\u001b[34m[algo-1:00212] shmem: mmap: an error occurred while determining whether or not /tmp/ompi.algo-1.0/jf.0/398917632/shared_mem_cuda_pool.algo-1 could be created.\u001b[0m\n",
      "\u001b[34m[algo-1:00212] create_and_attach: unable to create shared memory BTL coordinating structure :: size 134217728\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "train_s3_uri = f\"s3://{bucket}/train\"\n",
    "test_s3_uri = f\"s3://{bucket}/test\"\n",
    "\n",
    "inputs = {\n",
    "    \"training\": TrainingInput(\n",
    "        s3_data=train_s3_uri,\n",
    "        s3_data_type=\"S3Prefix\",\n",
    "        content_type=\"application/x-parquet\",\n",
    "        input_mode=\"File\"\n",
    "    ),\n",
    "    \"test\": TrainingInput(\n",
    "        s3_data=test_s3_uri,\n",
    "        s3_data_type=\"S3Prefix\",\n",
    "        content_type=\"application/x-parquet\",\n",
    "        input_mode=\"File\"\n",
    "    )\n",
    "}\n",
    "# (Optional) if you kept train/test together under the same prefix and rely purely on keyword,\n",
    "# you only need one channel. If you prefer a separate channel for test, you can add another:\n",
    "# inputs[\"test\"] = TrainingInput(s3_data=f\"s3://{bucket}/{test_prefix}/\", ...)\n",
    "\n",
    "# -----------------------------------\n",
    "# Estimator: Managed PyTorch DLC\n",
    "# -----------------------------------\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",          # your training script\n",
    "    source_dir=\".\",                  # folder containing train.py (and any utils/requirements.txt)\n",
    "    role=role,\n",
    "    framework_version=\"2.1.0\",       # pick a supported version\n",
    "    py_version=\"py310\",\n",
    "    instance_type=instance_type,\n",
    "    instance_count=instance_count,\n",
    "    dependencies = ['requirements.txt', 'helper_functions.py'],\n",
    "    hyperparameters=hyperparameters,\n",
    "    sagemaker_session=session,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    "    # max_run=max_run,\n",
    "    # use_spot_instances=use_spot,\n",
    "    # max_wait=max_wait if use_spot else None,\n",
    "    # Optional: checkpointing (recommended if using spot)\n",
    "    # checkpoint_s3_uri=f\"s3://{bucket}/checkpoints/autogluon-ts/\",\n",
    ")\n",
    "\n",
    "# (Optional) if you have a requirements.txt in the source_dir, PyTorch Estimator will install it.\n",
    "# Example requirements.txt lines that work for your script:\n",
    "# autogluon.timeseries[all]==1.1.1\n",
    "# pyarrow>=13.0.0\n",
    "# mlflow>=2.9.0\n",
    "# pandas>=2.0.0\n",
    "\n",
    "# -----------------------------------\n",
    "# Launch training\n",
    "# -----------------------------------\n",
    "job_name = sagemaker.utils.unique_name_from_base(\"ag-ts-train\")\n",
    "print(\"Starting training job:\", job_name)\n",
    "estimator.fit(inputs, job_name=job_name, wait=True, logs=True)\n",
    "\n",
    "# After completion:\n",
    "print(\"Model artifacts:\", estimator.model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7297fb71-8685-4a09-8b4c-b62576e1981b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
