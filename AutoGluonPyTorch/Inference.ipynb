{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62d83f53-4aa5-49d4-b7b3-6bccd7831b2b",
   "metadata": {},
   "source": [
    "### Deploy registered AG model in MLFlow for RT Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32478e28-6c4c-4f86-a3c4-0e31948734d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install fastavro avro -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b6dc839c-f680-4484-8d9d-01790601eafa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role, Session\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.predictor import Predictor\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import pandas as pd\n",
    "import io\n",
    "import fastavro\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sagemaker.serializers import BaseSerializer\n",
    "import cloudpickle\n",
    "from autogluon.timeseries import TimeSeriesDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fd45f877-41ec-437a-8fc3-692204ea2086",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'ag-example-timeseries'\n",
    "avro_prefix = 'avro-inf-stream'\n",
    "\n",
    "# Create S3 client\n",
    "s3 = boto3.client(\"s3\")\n",
    "prefix = 'mlflow-packaged-models'\n",
    "\n",
    "mlflow_experiment   = \"autogluon-timeseries\"\n",
    "region      = sagemaker.Session().boto_region_name\n",
    "session     = sagemaker.Session()\n",
    "role        = sagemaker.get_execution_role() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d126874-0079-4e6c-9cd5-3c3f95e62e77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Repacking model artifact (s3://sagemaker-us-east-1-543531862107/ag-ts-train-1757359159-6101/output/model.tar.gz), script artifact (ag_ts_source), and dependencies ([]) into single tar.gz file located at s3://sagemaker-us-east-1-543531862107/pytorch-inference-2025-09-09-23-00-47-296/model.tar.gz. This may take some time depending on model size...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing source directory for packaging...\n",
      "requirements.txt created.\n",
      "'ag_ts_inference.py' created in 'ag_ts_source' successfully.\n",
      "--- Starting Model Deployment ---\n",
      "Retrieving GPU image URI for ml.g4dn.2xlarge in us-east-1...\n",
      "Creating SageMaker Model object...\n",
      "Deploying model to endpoint: pytorch-inference-test with custom timeouts...\n",
      "This may take several minutes. Check the SageMaker console for status.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: pytorch-inference-2025-09-09-23-00-49-333\n",
      "INFO:sagemaker:Creating endpoint-config with name pytorch-inference-test\n",
      "INFO:sagemaker:Creating endpoint with name pytorch-inference-test\n"
     ]
    }
   ],
   "source": [
    "# --- Inference Script Content ---\n",
    "# The contents of ag_ts_inference.py are stored as a string to be written later.\n",
    "ag_ts_inference_script = \"\"\"\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import io\n",
    "import fastavro\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "\n",
    "# Required by SageMaker for loading the model\n",
    "def model_fn(model_dir):\n",
    "    \\\"\\\"\\\"\n",
    "    Loads the trained TimeSeriesPredictor from the model directory.\n",
    "    \\\"\\\"\\\"\n",
    "    try:\n",
    "        model_path = os.path.join(model_dir, \"model\")\n",
    "        print(f\"MODEL_LOG - Attempting to load model from: {model_path}\")\n",
    "        predictor = TimeSeriesPredictor.load(model_path)\n",
    "        print(\"MODEL_LOG - Model loaded successfully.\")\n",
    "        return predictor\n",
    "    except Exception as e:\n",
    "        print(f\"MODEL_LOG - Error loading model: {e}\")\n",
    "        # Re-raise the exception to fail the health check\n",
    "        raise\n",
    "\n",
    "# Required by SageMaker for inference\n",
    "def transform_fn(predictor, data, content_type, accept_type):\n",
    "    \\\"\\\"\\\"\n",
    "    Handles data deserialization, prediction, and serialization.\n",
    "    \\\"\\\"\\\"\n",
    "    print(f\"TRANSFORM_LOG - Received data with content_type: {content_type}\")\n",
    "    try:\n",
    "        # 1. Deserialize input based on the content_type\n",
    "        if content_type == \"application/json\":\n",
    "            df = pd.read_json(io.StringIO(data), orient=\"split\")\n",
    "        elif content_type == \"application/x-avro-bytes\":\n",
    "            input_stream = io.BytesIO(data)\n",
    "            reader = fastavro.reader(input_stream)\n",
    "            records = [r for r in reader]\n",
    "            df = pd.DataFrame.from_records(records)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported content type: {content_type}\")\n",
    "        \n",
    "        # Log deserialization details for debugging\n",
    "        print(f\"TRANSFORM_LOG - Deserialized data shape: {df.shape}\")\n",
    "        print(\"TRANSFORM_LOG - Deserialized data head:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # 2. Perform prediction on the deserialized data\n",
    "        ts_dataframe = TimeSeriesDataFrame(df)\n",
    "        predictions = predictor.predict(ts_dataframe)\n",
    "        \n",
    "        # 3. Serialize output based on the accept_type\n",
    "        if accept_type == \"application/json\":\n",
    "            # Return pandas-split JSON and the correct content type\n",
    "            return predictions.to_json(orient=\"split\"), accept_type\n",
    "        elif accept_type == \"text/csv\":\n",
    "            # Return CSV string and the correct content type\n",
    "            return predictions.to_csv(), accept_type\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported accept type: {accept_type}\")\n",
    "    except Exception as e:\n",
    "        print(f\"TRANSFORM_LOG - Error during transformation: {e}\")\n",
    "        raise\n",
    "\"\"\"\n",
    "\n",
    "# --- Custom Serializer and Predictor Classes ---\n",
    "# These are placed here so the notebook can use them\n",
    "class TimeSeriesAvroSerializer(BaseSerializer):\n",
    "    CONTENT_TYPE = \"application/x-avro-bytes\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def serialize(self, data):\n",
    "        schema = {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"record\",\n",
    "                \"name\": \"timeseries\",\n",
    "                \"fields\": [\n",
    "                    {\"name\": \"item_id\", \"type\": \"string\"},\n",
    "                    {\"name\": \"timestamp\", \"type\": \"string\"},\n",
    "                    {\"name\": \"target\", \"type\": \"float\"}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        records = []\n",
    "        for index, row in data.iterrows():\n",
    "            item_id = str(index[0])\n",
    "            timestamp = str(index[1])\n",
    "            target = float(row['target'])\n",
    "            records.append({\"item_id\": item_id, \"timestamp\": timestamp, \"target\": target})\n",
    "        \n",
    "        with io.BytesIO() as out:\n",
    "            fastavro.writer(out, schema, records)\n",
    "            return out.getvalue()\n",
    "\n",
    "class CustomPredictor(Predictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(CustomPredictor, self).__init__(\n",
    "            endpoint_name=endpoint_name,\n",
    "            sagemaker_session=sagemaker_session,\n",
    "            serializer=TimeSeriesAvroSerializer(),\n",
    "            deserializer=JSONDeserializer(),\n",
    "            content_type=\"application/x-avro-bytes\",\n",
    "            accept=\"application/json\"\n",
    "        )\n",
    "\n",
    "# --- SageMaker Configuration ---\n",
    "# Your AWS IAM role for SageMaker\n",
    "\n",
    "# The S3 URI pointing directly to the existing model.tar.gz from your training job's output.\n",
    "s3_uri_to_model_tar = \"s3://sagemaker-us-east-1-543531862107/ag-ts-train-1757359159-6101/output/model.tar.gz\"\n",
    "\n",
    "# The instance type for your endpoint\n",
    "instance_type = \"ml.g4dn.2xlarge\"\n",
    "\n",
    "# The name of the endpoint you are creating or updating\n",
    "endpoint_name = \"pytorch-inference-test\"\n",
    "\n",
    "# This is the local directory that will be packaged and uploaded to S3.\n",
    "source_dir = \"ag_ts_source\"\n",
    "inference_script = \"ag_ts_inference.py\"\n",
    "\n",
    "# --- Prepare Source Directory ---\n",
    "print(\"Preparing source directory for packaging...\")\n",
    "if os.path.exists(source_dir):\n",
    "    shutil.rmtree(source_dir)\n",
    "os.makedirs(source_dir, exist_ok=True)\n",
    "\n",
    "# Create the requirements.txt file\n",
    "with open(os.path.join(source_dir, \"requirements.txt\"), \"w\") as f:\n",
    "    f.write(\"autogluon.timeseries\\n\")\n",
    "    f.write(\"pandas\\n\")\n",
    "    f.write(\"fastavro\\n\")\n",
    "print(\"requirements.txt created.\")\n",
    "\n",
    "# Write the inference script content to the file\n",
    "with open(os.path.join(source_dir, inference_script), \"w\") as f:\n",
    "    f.write(ag_ts_inference_script)\n",
    "print(f\"'{inference_script}' created in '{source_dir}' successfully.\")\n",
    "\n",
    "# --- SageMaker Deployment ---\n",
    "print(\"--- Starting Model Deployment ---\")\n",
    "\n",
    "# Retrieve the URI for a PyTorch GPU inference image\n",
    "print(f\"Retrieving GPU image URI for {instance_type} in {region}...\")\n",
    "gpu_image_uri = image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=region,\n",
    "    version=\"2.2.0\",\n",
    "    py_version=\"py310\",\n",
    "    instance_type=instance_type,\n",
    "    image_scope=\"inference\",\n",
    ")\n",
    "\n",
    "# Create a SageMaker generic Model instance\n",
    "print(\"Creating SageMaker Model object...\")\n",
    "model = Model(\n",
    "    image_uri=gpu_image_uri,\n",
    "    model_data=s3_uri_to_model_tar,\n",
    "    role=role,\n",
    "    entry_point=inference_script,\n",
    "    source_dir=source_dir,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "# Deploy the model with extended timeouts\n",
    "print(f\"Deploying model to endpoint: {endpoint_name} with custom timeouts...\")\n",
    "print(\"This may take several minutes. Check the SageMaker console for status.\")\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    container_startup_health_check_timeout_seconds=300,  # 5 minutes\n",
    "    model_data_download_timeout=3600, # 1 hour\n",
    "    max_payload_in_mb=100\n",
    ")\n",
    "\n",
    "print(\"\\n--- Model Deployment Complete ---\")\n",
    "print(\"You can now proceed with inference tests.\")\n",
    "\n",
    "# --- Inference Test ---\n",
    "\n",
    "def generate_timeseries_df():\n",
    "    \"\"\"Generates a TimeSeriesDataFrame to match the expected schema.\"\"\"\n",
    "    data = {\n",
    "        'timestamp': pd.to_datetime(['2013-03-10 00:00:00', '2013-03-10 00:30:00']),\n",
    "        'target': [5200.0, 5220.0],\n",
    "        'item_id': ['T000000', 'T000000']\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    return TimeSeriesDataFrame(df.set_index(['item_id', 'timestamp']))\n",
    "\n",
    "print(\"\\n--- Running Inference Test ---\")\n",
    "try:\n",
    "    # 1. Instantiate the custom predictor\n",
    "    custom_predictor = CustomPredictor(endpoint_name=endpoint_name, sagemaker_session=sagemaker_session)\n",
    "\n",
    "    # 2. Generate and send the data to the endpoint\n",
    "    data_to_send = generate_timeseries_df()\n",
    "    print(\"Sending data to endpoint:\")\n",
    "    print(data_to_send)\n",
    "    \n",
    "    response_bytes = custom_predictor.predict(data_to_send)\n",
    "    \n",
    "    # 3. Process the response\n",
    "    response_json = json.loads(response_bytes)\n",
    "    \n",
    "    print(\"\\nPrediction received successfully:\")\n",
    "    print(json.dumps(response_json, indent=2))\n",
    "\n",
    "    # --- Plotting the result ---\n",
    "    results = []\n",
    "    for item_preds in response_json[\"data\"]:\n",
    "        # The response format will be [item_id, timestamp, prediction]\n",
    "        results.append({\n",
    "            \"item_id\": item_preds[0],\n",
    "            \"timestamp\": pd.to_datetime(item_preds[1]),\n",
    "            \"prediction\": item_preds[2]\n",
    "        })\n",
    "\n",
    "    if results:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        for item_id in results_df[\"item_id\"].unique():\n",
    "            item_df = results_df[results_df[\"item_id\"] == item_id]\n",
    "            ax.plot(item_df[\"timestamp\"], item_df[\"prediction\"], label=f\"Item {item_id}\")\n",
    "        \n",
    "        ax.set_title(\"Real-Time Autogluon TimeSeries Predictions\")\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_ylabel(\"Prediction\")\n",
    "        ax.legend()\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No valid prediction data to plot.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "183acd3d-ef66-4f3a-ac08-87921ac03ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4140bf0-c853-4448-b34e-178a5b2ec0e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
