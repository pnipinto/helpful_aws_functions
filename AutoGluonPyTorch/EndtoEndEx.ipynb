{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62d83f53-4aa5-49d4-b7b3-6bccd7831b2b",
   "metadata": {},
   "source": [
    "### Use the AG example in the docs (Forecasting with Chronos) and then convert to training container and inf container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "69a575bc-12e6-477d-8f8b-d17901927ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n",
    "import numpy as np\n",
    "import boto3\n",
    "import uuid\n",
    "from io import BytesIO\n",
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.session import TrainingInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d420423b-53bc-424e-aa29-04bb54267277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">T000000</th>\n",
       "      <th>2013-03-10 00:00:00</th>\n",
       "      <td>5207.959961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-10 00:30:00</th>\n",
       "      <td>5002.275879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-10 01:00:00</th>\n",
       "      <td>4747.569824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-10 01:30:00</th>\n",
       "      <td>4544.880859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-10 02:00:00</th>\n",
       "      <td>4425.952148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  target\n",
       "item_id timestamp                       \n",
       "T000000 2013-03-10 00:00:00  5207.959961\n",
       "        2013-03-10 00:30:00  5002.275879\n",
       "        2013-03-10 01:00:00  4747.569824\n",
       "        2013-03-10 01:30:00  4544.880859\n",
       "        2013-03-10 02:00:00  4425.952148"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = TimeSeriesDataFrame.from_path(\n",
    "    \"https://autogluon.s3.amazonaws.com/datasets/timeseries/australian_electricity_subset/test.csv\"\n",
    ")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cab1d3-92ca-47e3-884c-35637afa4531",
   "metadata": {},
   "source": [
    "### Add features (not in ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d64d03e6-c91b-4c31-9cda-275cfd11abe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>random_feature</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>item_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">T000000</th>\n",
       "      <th>2013-03-10 00:00:00</th>\n",
       "      <td>5207.959961</td>\n",
       "      <td>5558.006640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-10 00:30:00</th>\n",
       "      <td>5002.275879</td>\n",
       "      <td>5390.543877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-10 01:00:00</th>\n",
       "      <td>4747.569824</td>\n",
       "      <td>4798.287882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-10 01:30:00</th>\n",
       "      <td>4544.880859</td>\n",
       "      <td>4545.228370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-10 02:00:00</th>\n",
       "      <td>4425.952148</td>\n",
       "      <td>4706.047467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  target  random_feature\n",
       "item_id timestamp                                       \n",
       "T000000 2013-03-10 00:00:00  5207.959961     5558.006640\n",
       "        2013-03-10 00:30:00  5002.275879     5390.543877\n",
       "        2013-03-10 01:00:00  4747.569824     4798.287882\n",
       "        2013-03-10 01:30:00  4544.880859     4545.228370\n",
       "        2013-03-10 02:00:00  4425.952148     4706.047467"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_values = np.random.uniform(0, 500, size=len(data['target']))\n",
    "data['random_feature'] = data['target'].values + random_values\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b8349eed-dc1f-4820-8017-28f37030bc46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sorting the dataframe index before generating the train/test split.\n",
      "Beginning AutoGluon training...\n",
      "AutoGluon will save models to '/home/sagemaker-user/customer-demos/AutoGluonPyTorch/AutogluonModels/ag-20250904_071459'\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.12.9\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Aug 7 19:21:39 UTC 2025\n",
      "CPU Count:          48\n",
      "GPU Count:          4\n",
      "Memory Avail:       176.83 GB / 186.60 GB (94.8%)\n",
      "Disk Space Avail:   83.73 GB / 99.99 GB (83.7%)\n",
      "===================================================\n",
      "\n",
      "Fitting with arguments:\n",
      "{'enable_ensemble': True,\n",
      " 'eval_metric': WQL,\n",
      " 'hyperparameters': 'default',\n",
      " 'known_covariates_names': [],\n",
      " 'num_val_windows': 1,\n",
      " 'prediction_length': 48,\n",
      " 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
      " 'random_seed': 123,\n",
      " 'refit_every_n_windows': 1,\n",
      " 'refit_full': False,\n",
      " 'skip_model_selection': False,\n",
      " 'target': 'target',\n",
      " 'verbosity': 2}\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RangeIndex' object has no attribute 'levels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m train_data \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m      4\u001b[0m test_data \u001b[38;5;241m=\u001b[39m test_data\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m      6\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mTimeSeriesPredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprediction_length\u001b[49m\n\u001b[0;32m----> 7\u001b[0m \u001b[43m                               \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/autogluon/core/utils/decorators.py:31\u001b[0m, in \u001b[0;36munpack.<locals>._unpack_inner.<locals>._call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     30\u001b[0m     gargs, gkwargs \u001b[38;5;241m=\u001b[39m g(\u001b[38;5;241m*\u001b[39mother_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/autogluon/timeseries/predictor.py:712\u001b[0m, in \u001b[0;36mTimeSeriesPredictor.fit\u001b[0;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, hyperparameter_tune_kwargs, excluded_model_types, num_val_windows, val_step_size, refit_every_n_windows, refit_full, enable_ensemble, skip_model_selection, random_seed, verbosity)\u001b[0m\n\u001b[1;32m    709\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFitting with arguments:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    710\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpprint\u001b[38;5;241m.\u001b[39mpformat({k:\u001b[38;5;250m \u001b[39mv\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mk,\u001b[38;5;250m \u001b[39mv\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mfit_args\u001b[38;5;241m.\u001b[39mitems()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mv\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m})\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 712\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_and_prepare_data_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvided train_data has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_dataset_stats(train_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_step_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/autogluon/timeseries/predictor.py:307\u001b[0m, in \u001b[0;36mTimeSeriesPredictor._check_and_prepare_data_frame\u001b[0;34m(self, data, name)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreq \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;66;03m# Use all items for inferring the frequency\u001b[39;00m\n\u001b[0;32m--> 307\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreq \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_frequency\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_items\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_if_irregular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    310\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not provided and cannot be inferred. Please set the expected data \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    311\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency when creating the predictor with `TimeSeriesPredictor(freq=...)` or ensure that \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    312\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe data has a regular time index with `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.convert_frequency(freq=...)`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/autogluon/timeseries/dataset/ts_dataframe.py:510\u001b[0m, in \u001b[0;36mTimeSeriesDataFrame.infer_frequency\u001b[0;34m(self, num_items, raise_if_irregular)\u001b[0m\n\u001b[1;32m    507\u001b[0m         items_subset \u001b[38;5;241m=\u001b[39m all_item_ids\u001b[38;5;241m.\u001b[39mto_series()\u001b[38;5;241m.\u001b[39msample(n\u001b[38;5;241m=\u001b[39mnum_items, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m123\u001b[39m)\n\u001b[1;32m    508\u001b[0m         df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[items_subset]\n\u001b[0;32m--> 510\u001b[0m candidate_freq \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlevels\u001b[49m[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mfreq\n\u001b[1;32m    511\u001b[0m index_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mto_frame(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_freq\u001b[39m(series: pd\u001b[38;5;241m.\u001b[39mSeries) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RangeIndex' object has no attribute 'levels'"
     ]
    }
   ],
   "source": [
    "prediction_length = 48\n",
    "train_data, test_data = data.train_test_split(prediction_length)\n",
    "train_data = train_data.reset_index()\n",
    "test_data = test_data.reset_index()\n",
    "\n",
    "predictor = TimeSeriesPredictor(prediction_length=prediction_length\n",
    "                               ).fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a930493-6870-422b-bda7-51ae9e96e79a",
   "metadata": {},
   "source": [
    "### Write files to parquet (not in ex.) to test train and inf job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "183acd3d-ef66-4f3a-ac08-87921ac03ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'ag-example-timeseries'\n",
    "\n",
    "# Create S3 client\n",
    "s3 = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "94bc8505-31f0-4d9c-aa9e-560239998d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>target</th>\n",
       "      <th>random_feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T000000</td>\n",
       "      <td>2013-03-10 00:00:00</td>\n",
       "      <td>5207.959961</td>\n",
       "      <td>5558.006640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T000000</td>\n",
       "      <td>2013-03-10 00:30:00</td>\n",
       "      <td>5002.275879</td>\n",
       "      <td>5390.543877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T000000</td>\n",
       "      <td>2013-03-10 01:00:00</td>\n",
       "      <td>4747.569824</td>\n",
       "      <td>4798.287882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T000000</td>\n",
       "      <td>2013-03-10 01:30:00</td>\n",
       "      <td>4544.880859</td>\n",
       "      <td>4545.228370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T000000</td>\n",
       "      <td>2013-03-10 02:00:00</td>\n",
       "      <td>4425.952148</td>\n",
       "      <td>4706.047467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172795</th>\n",
       "      <td>T000004</td>\n",
       "      <td>2015-02-27 21:30:00</td>\n",
       "      <td>368.948792</td>\n",
       "      <td>452.005475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172796</th>\n",
       "      <td>T000004</td>\n",
       "      <td>2015-02-27 22:00:00</td>\n",
       "      <td>346.332764</td>\n",
       "      <td>598.047604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172797</th>\n",
       "      <td>T000004</td>\n",
       "      <td>2015-02-27 22:30:00</td>\n",
       "      <td>327.962677</td>\n",
       "      <td>499.156166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172798</th>\n",
       "      <td>T000004</td>\n",
       "      <td>2015-02-27 23:00:00</td>\n",
       "      <td>307.481934</td>\n",
       "      <td>498.390110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172799</th>\n",
       "      <td>T000004</td>\n",
       "      <td>2015-02-27 23:30:00</td>\n",
       "      <td>291.532776</td>\n",
       "      <td>588.931326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>172800 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        item_id           timestamp       target  random_feature\n",
       "0       T000000 2013-03-10 00:00:00  5207.959961     5558.006640\n",
       "1       T000000 2013-03-10 00:30:00  5002.275879     5390.543877\n",
       "2       T000000 2013-03-10 01:00:00  4747.569824     4798.287882\n",
       "3       T000000 2013-03-10 01:30:00  4544.880859     4545.228370\n",
       "4       T000000 2013-03-10 02:00:00  4425.952148     4706.047467\n",
       "...         ...                 ...          ...             ...\n",
       "172795  T000004 2015-02-27 21:30:00   368.948792      452.005475\n",
       "172796  T000004 2015-02-27 22:00:00   346.332764      598.047604\n",
       "172797  T000004 2015-02-27 22:30:00   327.962677      499.156166\n",
       "172798  T000004 2015-02-27 23:00:00   307.481934      498.390110\n",
       "172799  T000004 2015-02-27 23:30:00   291.532776      588.931326\n",
       "\n",
       "[172800 rows x 4 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "55e42fb1-e4f1-428b-b3f1-bd2458d77bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "# Number of parquet files to create\n",
    "num_files = 100\n",
    "\n",
    "dfs_to_write = {'train': train_data, 'test': test_data}\n",
    "\n",
    "for key in dfs_to_write.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9316b046-ef8c-417c-a19a-5903a440809b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://ag-example-timeseries/train/dummy_0200e716.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_06031b87.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_0239602b.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_0a8314ae.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_15a7f40a.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_175c1289.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_1b96077d.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_29959034.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_1f582372.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_0e66365e.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_0ea345a1.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_2c548bed.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_12e32cb2.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_118bf4d5.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_29bbdb66.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_110f5551.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_2d91bb12.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_2db70a15.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_2dfcd882.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_3127306d.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_419ee0a2.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_36271834.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_41e24430.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_2ecdfaee.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_39c673fb.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_45e7508f.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_464d6920.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_5632d269.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_534c6f30.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_568fe372.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_58236fdc.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_66b83c9c.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_6a4606c3.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_6ab365e8.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_6b4b8b34.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_6c61967a.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_6d886105.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_6e6eca5d.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_0babc74f.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_5e461c0e.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_7cb135d4.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_7cf14721.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_7d91c069.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_7d044859.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_7d94af91.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_7f2a288a.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_82dbe065.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_8337e69c.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_867cd64f.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_89d2be77.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_8ce5d805.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_8b5854b8.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_7bb3f34d.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_8e531365.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_8fce651e.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_8fffe82b.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_95f0fcdf.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_a35f3b59.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_a6fd21c6.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_94f0044d.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_a8dc4565.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_ad2aa419.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_b19d3d0b.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_b15fb16a.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_ae3dbbd5.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_8331320b.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_bb13c22e.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_bb195b17.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_bc644e65.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_bfbec329.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_be256337.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_c7a53cb2.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_cb57288c.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_ce1812a5.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_cb9e1630.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_d649d808.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_d2585251.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_df56d94d.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_dcfd3de7.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_dfeac3b7.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_e22a8e18.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_e281de67.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_e3cf81c8.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_dc3fbd63.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_e3ff1556.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_d55d8d66.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_ea38d210.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_e4639576.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_f5d4187c.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_f3d860a9.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_ee6c96b3.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_f9d85dc0.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_f9f3612f.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_fd46ab53.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_fdc91033.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_ffd74ebc.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_20b1f4cf.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_2438319f.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_23818de3.parquet\n",
      "delete: s3://ag-example-timeseries/train/dummy_229b61ba.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_00a31511.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_02a143b3.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_0f1c2a53.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_039f8747.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_14ce73cb.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_06479148.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_104bb12b.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_1169a9de.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_0ac51389.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_13c6cd2b.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_152033fd.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_13083f64.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_1583fdc5.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_1f355079.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_271fe93d.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_270bfd55.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_32587148.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_17d09108.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_2f29b039.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_3271e95f.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_3cbf6b6f.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_391d8aea.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_292c9730.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_3e0cebe9.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_3f1ada07.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_407d5b36.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_41cf33d1.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_462bdc5d.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_45fd2102.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_463fe503.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_4f731d6a.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_4e50a077.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_4d7abb2c.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_509b630f.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_5683e0bb.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_50217752.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_52b26133.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_5b5acb8e.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_5d6e75ca.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_585fb24c.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_5fd4d174.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_61961de7.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_722f92ba.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_6cc4c656.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_52343e68.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_7b6bd808.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_748f427e.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_7dd620bf.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_7faa87be.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_81137382.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_82b8ee9e.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_86ca7a86.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_8df7d32e.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_8e5f3124.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_8ef9014b.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_923e00f3.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_9341c7a7.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_9695a235.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_96fefba4.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_9a874612.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_9771707f.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_9d0f68f9.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_9d54fead.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_9f68bcd6.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_ae0846ce.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_af493442.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_b67e06e3.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_b67c3315.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_190ba684.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_ba04fd87.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_b1df1b59.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_bde834f0.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_c0314caf.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_c75e45c3.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_c483710b.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_c7e1d7d7.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_c7a4ec54.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_ca8c849c.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_d71b0e60.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_d80ef2fd.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_da6990b8.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_dbffafd3.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_da018d0b.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_dc1255c7.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_e3989e37.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_dcaa462d.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_e9d16ae5.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_ef363e03.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_ee71b864.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_f21d3cbf.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_f26afcdb.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_f7e3f19a.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_f75bc749.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_ef0a19da.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_fa16efaa.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_19ee4c1e.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_1b498e84.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_bffd4eb7.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_1efe0fa2.parquet\n",
      "delete: s3://ag-example-timeseries/test/dummy_f7f8d33b.parquet\n"
     ]
    }
   ],
   "source": [
    "# Clear train & test buckets [can change this]\n",
    "! aws s3 rm s3://{bucket}/train/ --recursive \n",
    "! aws s3 rm s3://{bucket}/test/ --recursive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ae127969-e2c7-46e9-a5d3-02474206e21f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'TimeSeriesDataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'TimeSeriesDataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_5bbb3399.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_c3eb8da8.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_64764251.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_5ddcd6f5.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_5ef7709c.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_67347828.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_acb07ec2.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_bf0b0f1d.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_e9a3d22b.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_762a5997.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_2e8d7323.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_9c9f21c0.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_e29e9280.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_11711ae8.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_5c260136.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_28c0ef4e.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_34f3218d.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_67ac3736.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_6c3d5716.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_e11ae8db.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_17ef5cc9.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_e25ba004.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_6cb82bdf.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_c212470a.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_3f6b206f.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_199f7825.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_8bf2ceea.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_05246c34.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_1fd9d98d.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_e97c0b04.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_9ebd869a.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_d0c07150.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_1a75da24.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_a41d8d3a.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_ef98f533.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_baea8b9c.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_fea4c1e1.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_db31d0de.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_c3f603b0.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_0fcb28fe.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_9a764ea8.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_b465f1ad.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_dbd33219.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_5368598a.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_66fc44a8.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_70203ca7.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_b358e145.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_cf9b4b35.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_b3a07756.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_a3bad49f.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_ae00e515.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_063c3e10.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_d4478378.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_1228d43d.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_c7465267.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_4a6d449c.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_f7d29dcc.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_b28cdc35.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_2cb0fe02.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_885d89f6.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_2c1e17bd.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_b524a010.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_d35c155a.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_bd90d875.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_10471392.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_287cde9f.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_c77e0007.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_43949a5b.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_d3fa8693.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_ff391d2b.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_b5ec04b8.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_29ebfcf1.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_09ea8721.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_7118e3d0.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_8fa05b01.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_fc02f0c4.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_33a1f0bb.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_27dbcb3f.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_88fc16de.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_52f216eb.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_70503b17.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_4ac561a8.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_5ea14ada.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_92d8bf19.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_808f209e.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_17bc78fc.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_ac7f0eb2.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_1ac993dd.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_8a937486.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_3c44e192.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_7c6616b6.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_9d296cac.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_be332ffa.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_542d88d4.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_333bfa3e.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_989abacf.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_4b895e23.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_bd91c9fb.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_7e910ad6.parquet\n",
      "Uploaded 1728 rows to s3://ag-example-timeseries/train/dummy_9db06450.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_000940ff.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'TimeSeriesDataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'TimeSeriesDataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_a8ee0419.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_f3d25011.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_cbf7a4cf.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_023e9270.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_4cd44d04.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_b816400d.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_7f6ae6de.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_13adb48d.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_07a3bc28.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_4a94b012.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_048d6dd1.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_605d34d7.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_16a66882.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_077af902.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_162d992d.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_2c5df371.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_233db148.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_a3537447.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_4209d28c.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_1ead1f21.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_b75bed34.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_e9f4c6b5.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_3111b5fb.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_37a17fec.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_28c72427.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_afcb9e01.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_82d8979c.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_e5ec1f0d.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_93550ee3.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_4d25d1f1.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_09ea5b1b.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_5597ce7f.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_c04ce3ad.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_b42ad01a.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_e05dae56.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_501b567e.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_5e09a281.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_06a406fe.parquet\n",
      "Uploaded 1731 rows to s3://ag-example-timeseries/test/dummy_1bf58705.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_37c98846.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_dcd73c09.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_c6c4df48.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_2b832058.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_ac944ca5.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_419aa6e9.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_5d44dec0.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_5abba4fb.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_e79342fa.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_492c1595.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_efe337f8.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_f8239f20.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_0303f1d7.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_acc947ba.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_be551ed3.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_33fec6bf.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_18153726.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_01f14faf.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_5b9c7f17.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_346202a3.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_78ed0bd9.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_9b62a940.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_bb32303b.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_010a9858.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_37bfb9dc.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_77ae776a.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_7900d465.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_c4db6c30.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_84c51e20.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_b95cac0d.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_31c3146a.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_dd312388.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_882094fa.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_dd43023a.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_d42becbc.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_38df46be.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_014a43a4.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_7454565f.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_6f4be693.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_c090562a.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_f2a8e4b0.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_1ee0db47.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_9547c90c.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_1345a3fa.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_35c03ea0.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_a3e1cef3.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_ab73542b.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_1d731a29.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_f933975c.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_bcadc09c.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_4d6cc5e9.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_e08a5537.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_3cfb8fba.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_8174af23.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_551cb2b8.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_e92f8b3e.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_63fef998.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_c137d3bc.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_3fee32a9.parquet\n",
      "Uploaded 1730 rows to s3://ag-example-timeseries/test/dummy_d586dc7e.parquet\n"
     ]
    }
   ],
   "source": [
    "for split_name, df in dfs_to_write.items():\n",
    "    # Shuffle and split into chunks\n",
    "    shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    chunks = np.array_split(shuffled, num_files)\n",
    "\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        filename = f\"dummy_{uuid.uuid4().hex[:8]}.parquet\"\n",
    "        s3_key = f\"{split_name}/{filename}\"   # <-- include folder + filename\n",
    "\n",
    "        buffer = BytesIO()\n",
    "        chunk_reset = chunk.reset_index()\n",
    "        chunk_reset.to_parquet(buffer, index=False)\n",
    "        buffer.seek(0)\n",
    "\n",
    "        s3.upload_fileobj(buffer, bucket, s3_key)\n",
    "        print(f\"Uploaded {len(chunk)} rows to s3://{bucket}/{s3_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b1059ada-8ef7-486c-bbc5-fa8a75f8978e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['item_id', 'timestamp', 'target', 'random_feature'], dtype='object')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "925a3230-a6e7-49de-b6e2-edd0f706e628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble\n"
     ]
    }
   ],
   "source": [
    "train_data\n",
    "predictions = predictor.predict(train_data)\n",
    "# predictor.plot(\n",
    "#     data=data,\n",
    "#     predictions=predictions,\n",
    "#     item_ids=data.item_ids[:2],\n",
    "#     max_history_length=200,\n",
    "# );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d7e7bd-6126-474e-bf6f-c7be6c4ea192",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model not specified in predict, will default to the model with the best validation score: WeightedEnsemble\n"
     ]
    }
   ],
   "source": [
    "# predictions = predictor.predict(train_data)\n",
    "# predictor.plot(\n",
    "#     data=data,\n",
    "#     predictions=predictions,\n",
    "#     item_ids=data.item_ids[:2],\n",
    "#     max_history_length=200,\n",
    "# );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31321511-788b-4a38-9946-a05431b44d1b",
   "metadata": {},
   "source": [
    "### Create train container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "14fcf0e4-c354-4814-a1de-8d91d22d1abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import argparse\n",
    "from functools import wraps\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import mlflow\n",
    "\n",
    "from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n",
    "from helper_functions import AGTimeSeriesWrapper   # keep this in your source_dir\n",
    "\n",
    "# ----------------------------\n",
    "# Retry helper\n",
    "# ----------------------------\n",
    "def retry_decorator(max_attempts=3, delay_seconds=60, backoff_factor=2):\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            attempts, delay = 0, delay_seconds\n",
    "            while attempts < max_attempts:\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    attempts += 1\n",
    "                    if attempts >= max_attempts:\n",
    "                        raise\n",
    "                    print(f\"[retry] {e} | attempt {attempts}/{max_attempts} | sleeping {delay}s\")\n",
    "                    time.sleep(delay)\n",
    "                    delay *= backoff_factor\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# ----------------------------\n",
    "# Args\n",
    "# ----------------------------\n",
    "def parse_args():\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument('--output_dir', type=str, default='/opt/ml/model')\n",
    "\n",
    "    # MLflow (managed tracking server ARN)\n",
    "    p.add_argument('--mlflow_arn', type=str, required=True)\n",
    "    p.add_argument('--mlflow_experiment', type=str, required=True)\n",
    "\n",
    "    # Data (SageMaker channel or custom path)\n",
    "    p.add_argument('--data_dir', type=str, default=os.environ.get('SM_CHANNEL_TRAINING', '/opt/ml/input/data/training'))\n",
    "    p.add_argument('--train-keyword', type=str, default=None)  # filter filenames; None => all\n",
    "    p.add_argument('--test-keyword', type=str, default=None)\n",
    "\n",
    "    # Schema\n",
    "    p.add_argument('--id-col', type=str, default='item_id')\n",
    "    p.add_argument('--time-col', type=str, default='timestamp')\n",
    "    p.add_argument('--target-col', type=str, default='target')\n",
    "\n",
    "    # Model\n",
    "    p.add_argument('--prediction-length', type=int, default=24)\n",
    "    p.add_argument('--eval-metric', type=str, default='MAPE')\n",
    "    p.add_argument('--presets', type=str, default='best_quality')\n",
    "    p.add_argument('--time-limit', type=int, default=900)\n",
    "    p.add_argument('--num-gpus', type=int, default=int(os.environ.get('SM_NUM_GPUS', '0')))\n",
    "    return p.parse_args()\n",
    "\n",
    "# ----------------------------\n",
    "# File discovery\n",
    "# ----------------------------\n",
    "def _find_parquet_files(root: str, keyword: str | None):\n",
    "    if not os.path.isdir(root):\n",
    "        raise FileNotFoundError(f\"Data directory not found: {root}\")\n",
    "    all_files = glob.glob(os.path.join(root, \"**\", \"*.parquet\"), recursive=True)\n",
    "    files = [f for f in all_files if (keyword in os.path.basename(f))] if keyword else all_files\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No parquet files in {root} matching keyword='{keyword}'\")\n",
    "    return sorted(files)\n",
    "\n",
    "# ----------------------------\n",
    "# Loader -> TSF objects (target + optional covariates)\n",
    "# ----------------------------\n",
    "@retry_decorator(max_attempts=3, delay_seconds=30, backoff_factor=2)\n",
    "def load_timeseries_parquet(\n",
    "    data_dir: str,\n",
    "    keyword: str | None,\n",
    "    id_col: str,\n",
    "    time_col: str,\n",
    "    target_col: str,\n",
    "    covariate_cols: list[str] | None = None,  # e.g. [\"random_feature\"]\n",
    "):\n",
    "    files = _find_parquet_files(data_dir, keyword)\n",
    "\n",
    "    def resolve(cols: list[str], desired: str, aliases: list[str]) -> str | None:\n",
    "        norm = {c.strip().lower(): c for c in cols}\n",
    "        for cand in [desired] + aliases:\n",
    "            k = cand.strip().lower()\n",
    "            if k in norm:\n",
    "                return norm[k]\n",
    "        return None\n",
    "\n",
    "    frames = []\n",
    "    for fp in files:\n",
    "        t = pq.read_table(fp)\n",
    "        df = t.to_pandas()\n",
    "        \n",
    "    all_df = pd.concat(frames, ignore_index=True)\n",
    "    all_df = all_df.sort_values([\"item_id\", \"timestamp\"]).reset_index(drop=True)\n",
    "\n",
    "    target_tsf = TimeSeriesDataFrame.from_data_frame(\n",
    "        all_df[[\"item_id\", \"timestamp\", \"target\"]],\n",
    "        id_column=\"item_id\",\n",
    "        timestamp_column=\"timestamp\",\n",
    "    )\n",
    "\n",
    "    cov_tsf = None\n",
    "    if covariate_cols:\n",
    "        present = [c for c in covariate_cols if c in all_df.columns]  # typically none unless you merged them in\n",
    "        if present:\n",
    "            cov_df = all_df[[\"item_id\", \"timestamp\"] + present]\n",
    "            cov_tsf = TimeSeriesDataFrame.from_data_frame(\n",
    "                cov_df, id_column=\"item_id\", timestamp_column=\"timestamp\"\n",
    "            )\n",
    "\n",
    "    return target_tsf, cov_tsf\n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    # Managed MLflow (plugin lets ARN be the URI)\n",
    "    mlflow.set_tracking_uri(args.mlflow_arn)\n",
    "    mlflow.set_experiment(args.mlflow_experiment)\n",
    "\n",
    "    print(f\"[load] train dir={args.data_dir} keyword={args.train_keyword!r}\")\n",
    "    train_tsf, train_cov_tsf = load_timeseries_parquet(\n",
    "        args.data_dir, args.train_keyword, args.id_col, args.time_col, args.target_col,\n",
    "        covariate_cols=[\"random_feature\"],\n",
    "    )\n",
    "\n",
    "    test_tsf = test_cov_tsf = None\n",
    "    if args.test_keyword not in (None, \"\", \"None\"):\n",
    "        try:\n",
    "            print(f\"[load] test dir={args.data_dir} keyword={args.test_keyword!r}\")\n",
    "            test_tsf, test_cov_tsf = load_timeseries_parquet(\n",
    "                args.data_dir, args.test_keyword, args.id_col, args.time_col, args.target_col,\n",
    "                covariate_cols=[\"random_feature\"],\n",
    "            )\n",
    "        except FileNotFoundError:\n",
    "            print(\"[load] no test parquet files found — skipping evaluation.\")\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params({\n",
    "            \"prediction_length\": args.prediction_length,\n",
    "            \"eval_metric\": args.eval_metric,\n",
    "            \"presets\": args.presets,\n",
    "            \"time_limit\": args.time_limit,\n",
    "            \"train_keyword\": args.train_keyword,\n",
    "            \"test_keyword\": args.test_keyword,\n",
    "        })\n",
    "\n",
    "        predictor = TimeSeriesPredictor(\n",
    "            prediction_length=args.prediction_length,\n",
    "            eval_metric=args.eval_metric,\n",
    "            path=args.output_dir,\n",
    "        )\n",
    "\n",
    "        predictor.fit(\n",
    "            train_data=train_tsf,\n",
    "            past_covariates=train_cov_tsf,   # treat random_feature as past covariate\n",
    "            presets=args.presets,\n",
    "            time_limit=args.time_limit,\n",
    "            num_gpus=args.num_gpus,\n",
    "        )\n",
    "        predictor.save()\n",
    "\n",
    "        if test_tsf is not None:\n",
    "            scores = predictor.evaluate(\n",
    "                test_tsf,\n",
    "                past_covariates=test_cov_tsf,\n",
    "            )\n",
    "            for k, v in scores.items():\n",
    "                mlflow.log_metric(f\"test_{k}\", float(v))\n",
    "\n",
    "        # PyFunc (deployable)\n",
    "        conda_env = {\n",
    "            \"name\": \"agts-env\",\n",
    "            \"channels\": [\"conda-forge\"],\n",
    "            \"dependencies\": [\n",
    "                \"python=3.10\",\n",
    "                {\"pip\": [\n",
    "                    \"autogluon.timeseries[all]==1.1.1\",\n",
    "                    \"pandas>=2.0.0\",\n",
    "                    \"pyarrow>=13.0.0\",\n",
    "                    \"mlflow>=2.9.0\",\n",
    "                    \"sagemaker-mlflow>=0.1.0\",\n",
    "                ]},\n",
    "            ],\n",
    "        }\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=\"model\",\n",
    "            python_model=AGTimeSeriesWrapper(),\n",
    "            artifacts={\"predictor\": args.output_dir},\n",
    "            conda_env=conda_env,\n",
    "        )\n",
    "        print(\"[done] training complete and model logged to MLflow.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc46ac60-838b-48c4-a10b-7f3a2f88270b",
   "metadata": {},
   "source": [
    "### Run the training container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7d390b35-610b-428e-ab6d-e3f9ebef0312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "region      = sagemaker.Session().boto_region_name\n",
    "session     = sagemaker.Session()\n",
    "role        = sagemaker.get_execution_role()  # or set your role arn string\n",
    "\n",
    "instance_type   = \"ml.g5.xlarge\"             # CPU example; use a GPU like \"ml.g5.2xlarge\" if needed\n",
    "instance_count  = 1\n",
    "use_spot        = True                        # optional cost saver\n",
    "max_wait        = 3600 + 600                  # seconds (must be > max_run if use_spot)\n",
    "max_run         = 3600                        # seconds\n",
    "\n",
    "# Hyperparameters for train.py (match argparse names)\n",
    "hyperparameters = {\n",
    "    \"id-col\": \"item_id\",\n",
    "    \"time-col\": \"timestamp\",\n",
    "    \"target-col\": \"target\",\n",
    "    # \"train-keyword\": None,                 # your parquet file name filter\n",
    "    # \"test-keyword\": None,                   # set None/\"\" if no test set\n",
    "    \"prediction-length\": 24,\n",
    "    \"eval-metric\": \"MAPE\",\n",
    "    \"presets\": \"best_quality\",\n",
    "    \"time-limit\": 900,                        # seconds\n",
    "    \"mlflow_arn\": 'arn:aws:sagemaker:us-east-1:543531862107:mlflow-tracking-server/ag-ex-timeseries',\n",
    "    \"mlflow_experiment\": \"autogluon-timeseries\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76615857-69b3-4540-b808-9dab6e29223f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training job: ag-ts-train-1757012283-4ffc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: ag-ts-train-1757012283-4ffc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-04 18:58:40 Starting - Starting the training job...\n",
      "2025-09-04 18:58:54 Starting - Preparing the instances for training............\n",
      "2025-09-04 19:01:12 Downloading - Downloading the training image................"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "train_s3_uri = f\"s3://{bucket}/train\"\n",
    "test_s3_uri = f\"s3://{bucket}/test\"\n",
    "\n",
    "inputs = {\n",
    "    \"training\": TrainingInput(\n",
    "        s3_data=train_s3_uri,\n",
    "        s3_data_type=\"S3Prefix\",\n",
    "        content_type=\"application/x-parquet\",\n",
    "        input_mode=\"File\"\n",
    "    ),\n",
    "    \"test\": TrainingInput(\n",
    "        s3_data=test_s3_uri,\n",
    "        s3_data_type=\"S3Prefix\",\n",
    "        content_type=\"application/x-parquet\",\n",
    "        input_mode=\"File\"\n",
    "    )\n",
    "}\n",
    "# (Optional) if you kept train/test together under the same prefix and rely purely on keyword,\n",
    "# you only need one channel. If you prefer a separate channel for test, you can add another:\n",
    "# inputs[\"test\"] = TrainingInput(s3_data=f\"s3://{bucket}/{test_prefix}/\", ...)\n",
    "\n",
    "# -----------------------------------\n",
    "# Estimator: Managed PyTorch DLC\n",
    "# -----------------------------------\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",          # your training script\n",
    "    source_dir=\".\",                  # folder containing train.py (and any utils/requirements.txt)\n",
    "    role=role,\n",
    "    framework_version=\"2.1.0\",       # pick a supported version\n",
    "    py_version=\"py310\",\n",
    "    instance_type=instance_type,\n",
    "    instance_count=instance_count,\n",
    "    dependencies = ['requirements.txt', 'helper_functions.py'],\n",
    "    hyperparameters=hyperparameters,\n",
    "    sagemaker_session=session,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    "    max_run=max_run,\n",
    "    use_spot_instances=use_spot,\n",
    "    max_wait=max_wait if use_spot else None,\n",
    "    # Optional: checkpointing (recommended if using spot)\n",
    "    # checkpoint_s3_uri=f\"s3://{bucket}/checkpoints/autogluon-ts/\",\n",
    ")\n",
    "\n",
    "# (Optional) if you have a requirements.txt in the source_dir, PyTorch Estimator will install it.\n",
    "# Example requirements.txt lines that work for your script:\n",
    "# autogluon.timeseries[all]==1.1.1\n",
    "# pyarrow>=13.0.0\n",
    "# mlflow>=2.9.0\n",
    "# pandas>=2.0.0\n",
    "\n",
    "# -----------------------------------\n",
    "# Launch training\n",
    "# -----------------------------------\n",
    "job_name = sagemaker.utils.unique_name_from_base(\"ag-ts-train\")\n",
    "print(\"Starting training job:\", job_name)\n",
    "estimator.fit(inputs, job_name=job_name, wait=True, logs=True)\n",
    "\n",
    "# After completion:\n",
    "print(\"Model artifacts:\", estimator.model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0219f26a-87b0-4af6-b8d6-0bb1e650dac2",
   "metadata": {},
   "source": [
    "### Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216337ee-b9cd-44c0-a10f-765590c1d4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ag_mlflow_pyfunc.py\n",
    "import json\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "from autogluon.timeseries import TimeSeriesPredictor, TimeSeriesDataFrame\n",
    "\n",
    "class AGTimeSeriesWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        # Load the saved AutoGluon predictor (artifact named \"predictor\")\n",
    "        self.predictor = TimeSeriesPredictor.load(context.artifacts[\"predictor\"])\n",
    "\n",
    "        # Load exogenous feature config if present\n",
    "        try:\n",
    "            with open(f'{context.artifacts[\"predictor\"]}/exog_config.json', \"r\") as f:\n",
    "                self.cfg = json.load(f)\n",
    "        except Exception:\n",
    "            self.cfg = {\"feature_name\": None, \"feature_role\": \"none\", \"feature_fill\": \"ffill\"}\n",
    "\n",
    "    def _extend_known(self, cov_tsf: TimeSeriesDataFrame, horizon: int, fill: str):\n",
    "        pdf = cov_tsf.to_pandas().sort_values([\"item_id\", \"timestamp\"])\n",
    "        vname = pdf.columns.difference([\"item_id\", \"timestamp\"])[0]\n",
    "        out = []\n",
    "        for item, g in pdf.groupby(\"item_id\"):\n",
    "            if len(g) < 2:\n",
    "                raise ValueError(f\"Cannot infer frequency for item {item}\")\n",
    "            freq = g[\"timestamp\"].iloc[1] - g[\"timestamp\"].iloc[0]\n",
    "            fut_idx = pd.date_range(g[\"timestamp\"].max() + freq, periods=horizon, freq=freq)\n",
    "            if fill == \"ffill\":\n",
    "                vals = [g[vname].iloc[-1]] * horizon\n",
    "            elif fill == \"zero\":\n",
    "                vals = [0.0] * horizon\n",
    "            elif fill == \"mean\":\n",
    "                vals = [float(g[vname].mean())] * horizon\n",
    "            else:\n",
    "                vals = [g[vname].iloc[-1]] * horizon\n",
    "            fut = pd.DataFrame({\"item_id\": item, \"timestamp\": fut_idx, vname: vals})\n",
    "            out.append(pd.concat([g, fut], ignore_index=True))\n",
    "        cov_ext = pd.concat(out, ignore_index=True)\n",
    "        return TimeSeriesDataFrame.from_data_frame(cov_ext, id_column=\"item_id\", timestamp_column=\"timestamp\")\n",
    "\n",
    "    def predict(self, context, model_input: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Expect columns: item_id, timestamp, optional target, optional random_feature\n",
    "        df = model_input.copy()\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "\n",
    "        # Base (target optional at inference)\n",
    "        cols = [c for c in [\"item_id\", \"timestamp\", \"target\"] if c in df.columns]\n",
    "        tsf = TimeSeriesDataFrame.from_data_frame(df[cols], id_column=\"item_id\", timestamp_column=\"timestamp\")\n",
    "\n",
    "        # Exogenous handling\n",
    "        feature = self.cfg.get(\"feature_name\")\n",
    "        role    = self.cfg.get(\"feature_role\", \"none\")\n",
    "        fill    = self.cfg.get(\"feature_fill\", \"ffill\")\n",
    "\n",
    "        known = past = None\n",
    "        static = None\n",
    "        if feature and feature in df.columns:\n",
    "            cov = TimeSeriesDataFrame.from_data_frame(\n",
    "                df[[\"item_id\", \"timestamp\", feature]], id_column=\"item_id\", timestamp_column=\"timestamp\"\n",
    "            )\n",
    "            if role == \"past\":\n",
    "                past = cov\n",
    "            elif role == \"known\":\n",
    "                known = self._extend_known(cov, self.predictor.prediction_length, fill)\n",
    "            elif role == \"static\":\n",
    "                static = df.groupby(\"item_id\")[feature].last().to_frame(name=feature)\n",
    "                static.index.name = \"item_id\"\n",
    "\n",
    "        forecast = self.predictor.predict(\n",
    "            tsf, known_covariates=known, past_covariates=past, static_features=static\n",
    "        )\n",
    "        # Return flat frame: item_id, timestamp, mean, (quantiles if available)\n",
    "        return forecast.to_pandas().reset_index()\n",
    "\n",
    "\n",
    "def log_ag_pyfunc(predictor_local_dir: str,\n",
    "                  experiment: str = \"autogluon-timeseries\",\n",
    "                  pip_deps = None) -> str:\n",
    "    \"\"\"\n",
    "    Logs an MLflow PyFunc model that wraps an AutoGluon predictor dir.\n",
    "    Returns model URI (runs:/<run_id>/model)\n",
    "    \"\"\"\n",
    "    if pip_deps is None:\n",
    "        pip_deps = [\n",
    "            \"autogluon.timeseries[all]==1.1.1\",\n",
    "            \"pandas>=2.0.0\",\n",
    "            \"pyarrow>=13.0.0\",\n",
    "            \"mlflow>=2.9.0\",\n",
    "        ]\n",
    "\n",
    "    mlflow.set_experiment(experiment)\n",
    "    with mlflow.start_run() as run:\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=\"model\",\n",
    "            python_model=AGTimeSeriesWrapper(),\n",
    "            artifacts={\"predictor\": predictor_local_dir},\n",
    "            conda_env={\n",
    "                \"name\": \"agts-env\",\n",
    "                \"channels\": [\"conda-forge\"],\n",
    "                \"dependencies\": [\n",
    "                    \"python=3.10\",\n",
    "                    {\"pip\": pip_deps}\n",
    "                ],\n",
    "            },\n",
    "        )\n",
    "        return f\"runs:/{run.info.run_id}/model\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7297fb71-8685-4a09-8b4c-b62576e1981b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
